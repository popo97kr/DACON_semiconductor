{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.254551</td>\n",
       "      <td>0.258823</td>\n",
       "      <td>0.254659</td>\n",
       "      <td>0.252085</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>0.253614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354750</td>\n",
       "      <td>0.369223</td>\n",
       "      <td>0.388184</td>\n",
       "      <td>0.408496</td>\n",
       "      <td>0.414564</td>\n",
       "      <td>0.429403</td>\n",
       "      <td>0.419225</td>\n",
       "      <td>0.443250</td>\n",
       "      <td>0.433414</td>\n",
       "      <td>0.465502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.205062</td>\n",
       "      <td>0.225544</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>0.202169</td>\n",
       "      <td>0.199633</td>\n",
       "      <td>0.207380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557203</td>\n",
       "      <td>0.573656</td>\n",
       "      <td>0.587998</td>\n",
       "      <td>0.612754</td>\n",
       "      <td>0.627825</td>\n",
       "      <td>0.633393</td>\n",
       "      <td>0.637706</td>\n",
       "      <td>0.625981</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.637853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.165869</td>\n",
       "      <td>0.177655</td>\n",
       "      <td>0.156822</td>\n",
       "      <td>0.175094</td>\n",
       "      <td>0.177755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699864</td>\n",
       "      <td>0.708688</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>0.713464</td>\n",
       "      <td>0.743030</td>\n",
       "      <td>0.741709</td>\n",
       "      <td>0.747743</td>\n",
       "      <td>0.746037</td>\n",
       "      <td>0.737356</td>\n",
       "      <td>0.750391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.120076</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.117931</td>\n",
       "      <td>0.130566</td>\n",
       "      <td>0.131262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764786</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.770017</td>\n",
       "      <td>0.787571</td>\n",
       "      <td>0.778866</td>\n",
       "      <td>0.776969</td>\n",
       "      <td>0.774712</td>\n",
       "      <td>0.801526</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.784057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.091033</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>0.108125</td>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.105917</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786677</td>\n",
       "      <td>0.802271</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.799614</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.787763</td>\n",
       "      <td>0.794948</td>\n",
       "      <td>0.819105</td>\n",
       "      <td>0.801781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer_1  layer_2  layer_3  layer_4         0         1         2         3  \\\n",
       "0       10       10       10       10  0.254551  0.258823  0.254659  0.252085   \n",
       "1       10       10       10       20  0.205062  0.225544  0.217758  0.202169   \n",
       "2       10       10       10       30  0.189196  0.165869  0.177655  0.156822   \n",
       "3       10       10       10       40  0.131003  0.120076  0.138975  0.117931   \n",
       "4       10       10       10       50  0.091033  0.086893  0.108125  0.080405   \n",
       "\n",
       "          4         5    ...          216       217       218       219  \\\n",
       "0  0.247678  0.253614    ...     0.354750  0.369223  0.388184  0.408496   \n",
       "1  0.199633  0.207380    ...     0.557203  0.573656  0.587998  0.612754   \n",
       "2  0.175094  0.177755    ...     0.699864  0.708688  0.721982  0.713464   \n",
       "3  0.130566  0.131262    ...     0.764786  0.763788  0.770017  0.787571   \n",
       "4  0.105917  0.077083    ...     0.786677  0.802271  0.806557  0.799614   \n",
       "\n",
       "        220       221       222       223       224       225  \n",
       "0  0.414564  0.429403  0.419225  0.443250  0.433414  0.465502  \n",
       "1  0.627825  0.633393  0.637706  0.625981  0.653231  0.637853  \n",
       "2  0.743030  0.741709  0.747743  0.746037  0.737356  0.750391  \n",
       "3  0.778866  0.776969  0.774712  0.801526  0.805305  0.784057  \n",
       "4  0.789333  0.804087  0.787763  0.794948  0.819105  0.801781  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.535410</td>\n",
       "      <td>0.520775</td>\n",
       "      <td>0.494087</td>\n",
       "      <td>0.465134</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.401751</td>\n",
       "      <td>0.355986</td>\n",
       "      <td>0.326427</td>\n",
       "      <td>0.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748339</td>\n",
       "      <td>0.757575</td>\n",
       "      <td>0.768130</td>\n",
       "      <td>0.777062</td>\n",
       "      <td>0.769173</td>\n",
       "      <td>0.768253</td>\n",
       "      <td>0.738704</td>\n",
       "      <td>0.739460</td>\n",
       "      <td>0.702139</td>\n",
       "      <td>0.702238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.351099</td>\n",
       "      <td>0.398179</td>\n",
       "      <td>0.413809</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>0.433257</td>\n",
       "      <td>0.455410</td>\n",
       "      <td>0.451065</td>\n",
       "      <td>0.464230</td>\n",
       "      <td>0.476011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333931</td>\n",
       "      <td>0.276307</td>\n",
       "      <td>0.211513</td>\n",
       "      <td>0.159223</td>\n",
       "      <td>0.110982</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.145420</td>\n",
       "      <td>0.260501</td>\n",
       "      <td>0.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490537</td>\n",
       "      <td>0.435958</td>\n",
       "      <td>0.413428</td>\n",
       "      <td>0.355796</td>\n",
       "      <td>0.335777</td>\n",
       "      <td>0.299944</td>\n",
       "      <td>0.242745</td>\n",
       "      <td>0.210555</td>\n",
       "      <td>0.180739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709371</td>\n",
       "      <td>0.746826</td>\n",
       "      <td>0.781436</td>\n",
       "      <td>0.788292</td>\n",
       "      <td>0.828630</td>\n",
       "      <td>0.835166</td>\n",
       "      <td>0.845859</td>\n",
       "      <td>0.846032</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.846779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>0.075802</td>\n",
       "      <td>0.133983</td>\n",
       "      <td>0.154546</td>\n",
       "      <td>0.209387</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.340617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.079884</td>\n",
       "      <td>0.147469</td>\n",
       "      <td>0.213112</td>\n",
       "      <td>0.298096</td>\n",
       "      <td>0.382823</td>\n",
       "      <td>0.489381</td>\n",
       "      <td>0.562383</td>\n",
       "      <td>0.599247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.154031</td>\n",
       "      <td>0.201728</td>\n",
       "      <td>0.270414</td>\n",
       "      <td>0.283799</td>\n",
       "      <td>0.343050</td>\n",
       "      <td>0.340233</td>\n",
       "      <td>0.379244</td>\n",
       "      <td>0.378511</td>\n",
       "      <td>0.373017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255070</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.271287</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>0.397950</td>\n",
       "      <td>0.486436</td>\n",
       "      <td>0.530573</td>\n",
       "      <td>0.582752</td>\n",
       "      <td>0.637296</td>\n",
       "      <td>0.637238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         0         1         2         3         4         5         6  \\\n",
       "0   0  0.535410  0.520775  0.494087  0.465134  0.430339  0.401751  0.355986   \n",
       "1   1  0.351099  0.398179  0.413809  0.418529  0.433257  0.455410  0.451065   \n",
       "2   2  0.490537  0.435958  0.413428  0.355796  0.335777  0.299944  0.242745   \n",
       "3   3  0.051634  0.075802  0.133983  0.154546  0.209387  0.251700  0.287552   \n",
       "4   4  0.154031  0.201728  0.270414  0.283799  0.343050  0.340233  0.379244   \n",
       "\n",
       "          7         8    ...          216       217       218       219  \\\n",
       "0  0.326427  0.282340    ...     0.748339  0.757575  0.768130  0.777062   \n",
       "1  0.464230  0.476011    ...     0.333931  0.276307  0.211513  0.159223   \n",
       "2  0.210555  0.180739    ...     0.709371  0.746826  0.781436  0.788292   \n",
       "3  0.333000  0.340617    ...     0.075046  0.056651  0.079884  0.147469   \n",
       "4  0.378511  0.373017    ...     0.255070  0.242396  0.271287  0.328828   \n",
       "\n",
       "        220       221       222       223       224       225  \n",
       "0  0.769173  0.768253  0.738704  0.739460  0.702139  0.702238  \n",
       "1  0.110982  0.083130  0.099780  0.145420  0.260501  0.343857  \n",
       "2  0.828630  0.835166  0.845859  0.846032  0.836724  0.846779  \n",
       "3  0.213112  0.298096  0.382823  0.489381  0.562383  0.599247  \n",
       "4  0.397950  0.486436  0.530573  0.582752  0.637296  0.637238  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.iloc[:,4:]\n",
    "train_Y = train.iloc[:,0:4]\n",
    "test_X = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>507466</th>\n",
       "      <td>0.374564</td>\n",
       "      <td>0.399940</td>\n",
       "      <td>0.433621</td>\n",
       "      <td>0.459866</td>\n",
       "      <td>0.472699</td>\n",
       "      <td>0.506335</td>\n",
       "      <td>0.507750</td>\n",
       "      <td>0.521975</td>\n",
       "      <td>0.533085</td>\n",
       "      <td>0.524141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559102</td>\n",
       "      <td>0.601892</td>\n",
       "      <td>0.645260</td>\n",
       "      <td>0.666463</td>\n",
       "      <td>0.686821</td>\n",
       "      <td>0.688790</td>\n",
       "      <td>0.693385</td>\n",
       "      <td>0.699150</td>\n",
       "      <td>0.688804</td>\n",
       "      <td>0.687256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703664</th>\n",
       "      <td>0.310897</td>\n",
       "      <td>0.244671</td>\n",
       "      <td>0.206002</td>\n",
       "      <td>0.177038</td>\n",
       "      <td>0.129345</td>\n",
       "      <td>0.090063</td>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.057348</td>\n",
       "      <td>0.060172</td>\n",
       "      <td>0.068039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445678</td>\n",
       "      <td>0.455155</td>\n",
       "      <td>0.502311</td>\n",
       "      <td>0.556557</td>\n",
       "      <td>0.600518</td>\n",
       "      <td>0.651768</td>\n",
       "      <td>0.681828</td>\n",
       "      <td>0.713912</td>\n",
       "      <td>0.709399</td>\n",
       "      <td>0.729241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263823</th>\n",
       "      <td>0.335130</td>\n",
       "      <td>0.350356</td>\n",
       "      <td>0.346621</td>\n",
       "      <td>0.374700</td>\n",
       "      <td>0.371109</td>\n",
       "      <td>0.368576</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.386184</td>\n",
       "      <td>0.385721</td>\n",
       "      <td>0.414943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868834</td>\n",
       "      <td>0.890896</td>\n",
       "      <td>0.879207</td>\n",
       "      <td>0.874428</td>\n",
       "      <td>0.857481</td>\n",
       "      <td>0.865239</td>\n",
       "      <td>0.852211</td>\n",
       "      <td>0.847266</td>\n",
       "      <td>0.834733</td>\n",
       "      <td>0.835734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175764</th>\n",
       "      <td>0.281411</td>\n",
       "      <td>0.271408</td>\n",
       "      <td>0.244176</td>\n",
       "      <td>0.194823</td>\n",
       "      <td>0.186489</td>\n",
       "      <td>0.144125</td>\n",
       "      <td>0.127492</td>\n",
       "      <td>0.120126</td>\n",
       "      <td>0.104159</td>\n",
       "      <td>0.097994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665061</td>\n",
       "      <td>0.700276</td>\n",
       "      <td>0.701411</td>\n",
       "      <td>0.731375</td>\n",
       "      <td>0.742891</td>\n",
       "      <td>0.770059</td>\n",
       "      <td>0.773033</td>\n",
       "      <td>0.766907</td>\n",
       "      <td>0.798111</td>\n",
       "      <td>0.802930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203528</th>\n",
       "      <td>0.379820</td>\n",
       "      <td>0.389189</td>\n",
       "      <td>0.426292</td>\n",
       "      <td>0.439232</td>\n",
       "      <td>0.473112</td>\n",
       "      <td>0.485637</td>\n",
       "      <td>0.484676</td>\n",
       "      <td>0.525790</td>\n",
       "      <td>0.531499</td>\n",
       "      <td>0.546377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552563</td>\n",
       "      <td>0.576524</td>\n",
       "      <td>0.616268</td>\n",
       "      <td>0.633502</td>\n",
       "      <td>0.661194</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.693648</td>\n",
       "      <td>0.696290</td>\n",
       "      <td>0.719072</td>\n",
       "      <td>0.737203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "507466  0.374564  0.399940  0.433621  0.459866  0.472699  0.506335  0.507750   \n",
       "703664  0.310897  0.244671  0.206002  0.177038  0.129345  0.090063  0.070579   \n",
       "263823  0.335130  0.350356  0.346621  0.374700  0.371109  0.368576  0.385600   \n",
       "175764  0.281411  0.271408  0.244176  0.194823  0.186489  0.144125  0.127492   \n",
       "203528  0.379820  0.389189  0.426292  0.439232  0.473112  0.485637  0.484676   \n",
       "\n",
       "               7         8         9    ...          216       217       218  \\\n",
       "507466  0.521975  0.533085  0.524141    ...     0.559102  0.601892  0.645260   \n",
       "703664  0.057348  0.060172  0.068039    ...     0.445678  0.455155  0.502311   \n",
       "263823  0.386184  0.385721  0.414943    ...     0.868834  0.890896  0.879207   \n",
       "175764  0.120126  0.104159  0.097994    ...     0.665061  0.700276  0.701411   \n",
       "203528  0.525790  0.531499  0.546377    ...     0.552563  0.576524  0.616268   \n",
       "\n",
       "             219       220       221       222       223       224       225  \n",
       "507466  0.666463  0.686821  0.688790  0.693385  0.699150  0.688804  0.687256  \n",
       "703664  0.556557  0.600518  0.651768  0.681828  0.713912  0.709399  0.729241  \n",
       "263823  0.874428  0.857481  0.865239  0.852211  0.847266  0.834733  0.835734  \n",
       "175764  0.731375  0.742891  0.770059  0.773033  0.766907  0.798111  0.802930  \n",
       "203528  0.633502  0.661194  0.675700  0.693648  0.696290  0.719072  0.737203  \n",
       "\n",
       "[5 rows x 226 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plain DNN (3 hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#케라스를 통해 모델 생성을 시작합니다.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(units=160, activation='relu', input_dim=226))\n",
    "model.add(Dense(units=160, activation='relu'))\n",
    "model.add(Dense(units=160, activation='relu'))\n",
    "model.add(Dense(units=4, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 567000 samples, validate on 243000 samples\n",
      "Epoch 1/50\n",
      "567000/567000 [==============================] - 19s 33us/step - loss: 50.4072 - mae: 50.4072 - val_loss: 50.2435 - val_mae: 50.2435\n",
      "Epoch 2/50\n",
      "567000/567000 [==============================] - 16s 28us/step - loss: 49.8918 - mae: 49.8918 - val_loss: 49.6706 - val_mae: 49.6706\n",
      "Epoch 3/50\n",
      "567000/567000 [==============================] - 14s 24us/step - loss: 49.4757 - mae: 49.4757 - val_loss: 49.2227 - val_mae: 49.2227\n",
      "Epoch 4/50\n",
      "567000/567000 [==============================] - 13s 23us/step - loss: 48.9695 - mae: 48.9695 - val_loss: 48.7710 - val_mae: 48.7710\n",
      "Epoch 5/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 48.5033 - mae: 48.5033 - val_loss: 48.5138 - val_mae: 48.5138\n",
      "Epoch 6/50\n",
      "567000/567000 [==============================] - 14s 24us/step - loss: 48.0376 - mae: 48.0376 - val_loss: 47.8956 - val_mae: 47.8956\n",
      "Epoch 7/50\n",
      "567000/567000 [==============================] - 13s 24us/step - loss: 47.6037 - mae: 47.6037 - val_loss: 47.4553 - val_mae: 47.4553\n",
      "Epoch 8/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 47.2251 - mae: 47.2251 - val_loss: 47.0604 - val_mae: 47.0604\n",
      "Epoch 9/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 46.8154 - mae: 46.8154 - val_loss: 46.7122 - val_mae: 46.7122\n",
      "Epoch 10/50\n",
      "567000/567000 [==============================] - 14s 24us/step - loss: 46.5094 - mae: 46.5094 - val_loss: 46.4676 - val_mae: 46.4676\n",
      "Epoch 11/50\n",
      "567000/567000 [==============================] - 13s 24us/step - loss: 46.1933 - mae: 46.1933 - val_loss: 46.2315 - val_mae: 46.2315\n",
      "Epoch 12/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 45.8483 - mae: 45.8484 - val_loss: 45.8393 - val_mae: 45.8393\n",
      "Epoch 13/50\n",
      "567000/567000 [==============================] - 16s 29us/step - loss: 45.5267 - mae: 45.5267 - val_loss: 45.4341 - val_mae: 45.4341\n",
      "Epoch 14/50\n",
      "567000/567000 [==============================] - 15s 27us/step - loss: 45.2775 - mae: 45.2775 - val_loss: 45.3353 - val_mae: 45.3354\n",
      "Epoch 15/50\n",
      "567000/567000 [==============================] - 15s 27us/step - loss: 44.9827 - mae: 44.9827 - val_loss: 44.9582 - val_mae: 44.9582\n",
      "Epoch 16/50\n",
      "567000/567000 [==============================] - 13s 24us/step - loss: 44.7287 - mae: 44.7287 - val_loss: 44.7653 - val_mae: 44.7653\n",
      "Epoch 17/50\n",
      "567000/567000 [==============================] - 16s 29us/step - loss: 44.4690 - mae: 44.4691 - val_loss: 44.5527 - val_mae: 44.5527\n",
      "Epoch 18/50\n",
      "567000/567000 [==============================] - 16s 29us/step - loss: 44.2619 - mae: 44.2619 - val_loss: 44.2844 - val_mae: 44.2844\n",
      "Epoch 19/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 44.0094 - mae: 44.0094 - val_loss: 44.0097 - val_mae: 44.0097\n",
      "Epoch 20/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 43.7034 - mae: 43.7034 - val_loss: 43.6914 - val_mae: 43.6914\n",
      "Epoch 21/50\n",
      "567000/567000 [==============================] - 15s 27us/step - loss: 43.5331 - mae: 43.5331 - val_loss: 43.7244 - val_mae: 43.7244\n",
      "Epoch 22/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 43.2980 - mae: 43.2980 - val_loss: 43.1776 - val_mae: 43.1776\n",
      "Epoch 23/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 43.0158 - mae: 43.0158 - val_loss: 42.9395 - val_mae: 42.9395\n",
      "Epoch 24/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 42.7788 - mae: 42.7788 - val_loss: 42.7715 - val_mae: 42.7715\n",
      "Epoch 25/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 42.5935 - mae: 42.5935 - val_loss: 42.9642 - val_mae: 42.9642\n",
      "Epoch 26/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 42.3682 - mae: 42.3682 - val_loss: 42.3566 - val_mae: 42.3566\n",
      "Epoch 27/50\n",
      "567000/567000 [==============================] - 16s 29us/step - loss: 42.1377 - mae: 42.1376 - val_loss: 42.1533 - val_mae: 42.1533\n",
      "Epoch 28/50\n",
      "567000/567000 [==============================] - 15s 27us/step - loss: 41.9633 - mae: 41.9633 - val_loss: 41.8956 - val_mae: 41.8956\n",
      "Epoch 29/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 41.6650 - mae: 41.6650 - val_loss: 41.6996 - val_mae: 41.6996\n",
      "Epoch 30/50\n",
      "567000/567000 [==============================] - 16s 29us/step - loss: 41.5347 - mae: 41.5347 - val_loss: 41.6252 - val_mae: 41.6252\n",
      "Epoch 31/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 41.2966 - mae: 41.2966 - val_loss: 41.3792 - val_mae: 41.3792\n",
      "Epoch 32/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 41.0922 - mae: 41.0922 - val_loss: 41.0380 - val_mae: 41.0380\n",
      "Epoch 33/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 40.9167 - mae: 40.9167 - val_loss: 41.1231 - val_mae: 41.1231\n",
      "Epoch 34/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 40.7615 - mae: 40.7615 - val_loss: 40.6744 - val_mae: 40.6744\n",
      "Epoch 35/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 40.5706 - mae: 40.5706 - val_loss: 40.5483 - val_mae: 40.5483\n",
      "Epoch 36/50\n",
      "567000/567000 [==============================] - 14s 24us/step - loss: 40.3698 - mae: 40.3698 - val_loss: 40.3629 - val_mae: 40.3629\n",
      "Epoch 37/50\n",
      "567000/567000 [==============================] - 17s 29us/step - loss: 40.1392 - mae: 40.1392 - val_loss: 40.2844 - val_mae: 40.2844\n",
      "Epoch 38/50\n",
      "567000/567000 [==============================] - 24s 42us/step - loss: 39.9945 - mae: 39.9945 - val_loss: 40.0034 - val_mae: 40.0034\n",
      "Epoch 39/50\n",
      "567000/567000 [==============================] - 18s 31us/step - loss: 39.8702 - mae: 39.8702 - val_loss: 39.9726 - val_mae: 39.9726\n",
      "Epoch 40/50\n",
      "567000/567000 [==============================] - 16s 29us/step - loss: 39.6962 - mae: 39.6962 - val_loss: 39.8053 - val_mae: 39.8053\n",
      "Epoch 41/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 39.5627 - mae: 39.5627 - val_loss: 39.5035 - val_mae: 39.5035\n",
      "Epoch 42/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 39.3847 - mae: 39.3847 - val_loss: 39.3798 - val_mae: 39.3798\n",
      "Epoch 43/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 39.1862 - mae: 39.1862 - val_loss: 39.2166 - val_mae: 39.2166\n",
      "Epoch 44/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 39.0773 - mae: 39.0773 - val_loss: 39.0960 - val_mae: 39.0960\n",
      "Epoch 45/50\n",
      "567000/567000 [==============================] - 16s 28us/step - loss: 38.9149 - mae: 38.9149 - val_loss: 39.0214 - val_mae: 39.0214\n",
      "Epoch 46/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 38.7840 - mae: 38.7840 - val_loss: 38.8724 - val_mae: 38.8724\n",
      "Epoch 47/50\n",
      "567000/567000 [==============================] - 14s 24us/step - loss: 38.6342 - mae: 38.6342 - val_loss: 38.7826 - val_mae: 38.7826\n",
      "Epoch 48/50\n",
      "567000/567000 [==============================] - 14s 25us/step - loss: 38.5089 - mae: 38.5089 - val_loss: 38.4965 - val_mae: 38.4965\n",
      "Epoch 49/50\n",
      "567000/567000 [==============================] - 15s 26us/step - loss: 38.3557 - mae: 38.3557 - val_loss: 38.3797 - val_mae: 38.3797\n",
      "Epoch 50/50\n",
      "567000/567000 [==============================] - 20s 34us/step - loss: 38.2749 - mae: 38.2749 - val_loss: 38.2605 - val_mae: 38.2605\n"
     ]
    }
   ],
   "source": [
    "mod=model.fit(train_X, train_Y, validation_data=(valid_X, valid_Y), epochs=50, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(mod.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(mod.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=160, input_dim=226))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=160))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=160))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=4, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510300 samples, validate on 56700 samples\n",
      "Epoch 1/50\n",
      "510300/510300 [==============================] - 76s 149us/step - loss: 152.5041 - mae: 152.5041 - val_loss: 153.7751 - val_mae: 153.7751\n",
      "Epoch 2/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 147.5330 - mae: 147.5330 - val_loss: 149.4398 - val_mae: 149.4398\n",
      "Epoch 3/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 141.0446 - mae: 141.0446 - val_loss: 136.5448 - val_mae: 136.5448\n",
      "Epoch 4/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 132.7100 - mae: 132.7100 - val_loss: 127.1441 - val_mae: 127.1441\n",
      "Epoch 5/50\n",
      "510300/510300 [==============================] - 21s 41us/step - loss: 122.5784 - mae: 122.5784 - val_loss: 120.6356 - val_mae: 120.6356\n",
      "Epoch 6/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 111.1736 - mae: 111.1736 - val_loss: 129.9176 - val_mae: 129.9176\n",
      "Epoch 7/50\n",
      "510300/510300 [==============================] - 21s 41us/step - loss: 99.0394 - mae: 99.0394 - val_loss: 137.0873 - val_mae: 137.0873\n",
      "Epoch 8/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 87.5293 - mae: 87.5293 - val_loss: 131.7970 - val_mae: 131.7970\n",
      "Epoch 9/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 78.2073 - mae: 78.2073 - val_loss: 105.4393 - val_mae: 105.4393\n",
      "Epoch 10/50\n",
      "510300/510300 [==============================] - 24s 47us/step - loss: 71.2782 - mae: 71.2782 - val_loss: 84.0046 - val_mae: 84.0046\n",
      "Epoch 11/50\n",
      "510300/510300 [==============================] - 28s 56us/step - loss: 66.0922 - mae: 66.0922 - val_loss: 78.9469 - val_mae: 78.9469\n",
      "Epoch 12/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 62.4875 - mae: 62.4875 - val_loss: 71.3066 - val_mae: 71.3066\n",
      "Epoch 13/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 59.8630 - mae: 59.8630 - val_loss: 66.8164 - val_mae: 66.8164\n",
      "Epoch 14/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 57.4559 - mae: 57.4559 - val_loss: 64.7011 - val_mae: 64.7011\n",
      "Epoch 15/50\n",
      "510300/510300 [==============================] - 21s 41us/step - loss: 54.9444 - mae: 54.9444 - val_loss: 66.0918 - val_mae: 66.0918\n",
      "Epoch 16/50\n",
      "510300/510300 [==============================] - 24s 47us/step - loss: 52.4780 - mae: 52.4780 - val_loss: 63.0435 - val_mae: 63.0435\n",
      "Epoch 17/50\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 50.0913 - mae: 50.0913 - val_loss: 60.2830 - val_mae: 60.2830\n",
      "Epoch 18/50\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 47.8007 - mae: 47.8007 - val_loss: 58.9381 - val_mae: 58.9382\n",
      "Epoch 19/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 45.7162 - mae: 45.7162 - val_loss: 56.3129 - val_mae: 56.3129\n",
      "Epoch 20/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 43.7549 - mae: 43.7549 - val_loss: 53.3556 - val_mae: 53.3556\n",
      "Epoch 21/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 41.9526 - mae: 41.9526 - val_loss: 50.5787 - val_mae: 50.5787\n",
      "Epoch 22/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 40.3062 - mae: 40.3062 - val_loss: 47.8535 - val_mae: 47.8535\n",
      "Epoch 23/50\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 38.8375 - mae: 38.8375 - val_loss: 45.5138 - val_mae: 45.5138\n",
      "Epoch 24/50\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 37.5213 - mae: 37.5213 - val_loss: 42.9306 - val_mae: 42.9306\n",
      "Epoch 25/50\n",
      "510300/510300 [==============================] - 23s 44us/step - loss: 36.2313 - mae: 36.2313 - val_loss: 42.6638 - val_mae: 42.6638\n",
      "Epoch 26/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 35.0114 - mae: 35.0114 - val_loss: 40.2314 - val_mae: 40.2314\n",
      "Epoch 27/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 33.8524 - mae: 33.8524 - val_loss: 39.8954 - val_mae: 39.8954\n",
      "Epoch 28/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 32.7926 - mae: 32.7927 - val_loss: 37.4034 - val_mae: 37.4034\n",
      "Epoch 29/50\n",
      "510300/510300 [==============================] - 23s 46us/step - loss: 31.9037 - mae: 31.9037 - val_loss: 36.3354 - val_mae: 36.3354\n",
      "Epoch 30/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 30.9695 - mae: 30.9695 - val_loss: 34.8375 - val_mae: 34.8375\n",
      "Epoch 31/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 30.2279 - mae: 30.2279 - val_loss: 33.4547 - val_mae: 33.4547\n",
      "Epoch 32/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 29.5615 - mae: 29.5615 - val_loss: 35.0978 - val_mae: 35.0978\n",
      "Epoch 33/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 29.0185 - mae: 29.0185 - val_loss: 32.4408 - val_mae: 32.4408\n",
      "Epoch 34/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 28.4568 - mae: 28.4568 - val_loss: 32.3535 - val_mae: 32.3535\n",
      "Epoch 35/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 28.0360 - mae: 28.0360 - val_loss: 30.7223 - val_mae: 30.7223\n",
      "Epoch 36/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 27.5596 - mae: 27.5596 - val_loss: 30.2320 - val_mae: 30.2320\n",
      "Epoch 37/50\n",
      "510300/510300 [==============================] - 21s 41us/step - loss: 27.2012 - mae: 27.2012 - val_loss: 29.2825 - val_mae: 29.2825\n",
      "Epoch 38/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 26.7682 - mae: 26.7682 - val_loss: 30.6917 - val_mae: 30.6917\n",
      "Epoch 39/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 26.6000 - mae: 26.6000 - val_loss: 28.6804 - val_mae: 28.6804\n",
      "Epoch 40/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 26.1682 - mae: 26.1682 - val_loss: 28.3570 - val_mae: 28.3570\n",
      "Epoch 41/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 25.9057 - mae: 25.9057 - val_loss: 27.0777 - val_mae: 27.0777\n",
      "Epoch 42/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 25.6265 - mae: 25.6265 - val_loss: 27.6284 - val_mae: 27.6284\n",
      "Epoch 43/50\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 25.5184 - mae: 25.5184 - val_loss: 27.5870 - val_mae: 27.5870\n",
      "Epoch 44/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 25.1467 - mae: 25.1467 - val_loss: 26.8066 - val_mae: 26.8066\n",
      "Epoch 45/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 25.0211 - mae: 25.0211 - val_loss: 26.5116 - val_mae: 26.5116\n",
      "Epoch 46/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 24.7905 - mae: 24.7905 - val_loss: 27.1220 - val_mae: 27.1220\n",
      "Epoch 47/50\n",
      "510300/510300 [==============================] - 22s 43us/step - loss: 24.7874 - mae: 24.7874 - val_loss: 26.5548 - val_mae: 26.5548\n",
      "Epoch 48/50\n",
      "510300/510300 [==============================] - 21s 42us/step - loss: 24.4604 - mae: 24.4604 - val_loss: 26.3413 - val_mae: 26.3413\n",
      "Epoch 49/50\n",
      "510300/510300 [==============================] - 22s 42us/step - loss: 24.2693 - mae: 24.2693 - val_loss: 25.8724 - val_mae: 25.8724\n",
      "Epoch 50/50\n",
      "510300/510300 [==============================] - 22s 44us/step - loss: 24.1468 - mae: 24.1468 - val_loss: 26.2716 - val_mae: 26.2716\n"
     ]
    }
   ],
   "source": [
    "mod=model.fit(train_X, train_Y, epochs=50, batch_size=10000, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(mod.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(mod.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With BN + He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=160, input_dim=226, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=4, kernel_initializer='he_normal', activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 510300 samples, validate on 56700 samples\n",
      "Epoch 1/500\n",
      "510300/510300 [==============================] - 108s 211us/step - loss: 153.1786 - mae: 153.1786 - val_loss: 153.3423 - val_mae: 153.3423\n",
      "Epoch 2/500\n",
      "510300/510300 [==============================] - 31s 61us/step - loss: 148.4265 - mae: 148.4265 - val_loss: 148.8255 - val_mae: 148.8255\n",
      "Epoch 3/500\n",
      "510300/510300 [==============================] - 29s 56us/step - loss: 141.9523 - mae: 141.9523 - val_loss: 137.1155 - val_mae: 137.1155\n",
      "Epoch 4/500\n",
      "510300/510300 [==============================] - 27s 54us/step - loss: 133.6224 - mae: 133.6224 - val_loss: 122.6756 - val_mae: 122.6756\n",
      "Epoch 5/500\n",
      "510300/510300 [==============================] - 29s 57us/step - loss: 123.5793 - mae: 123.5793 - val_loss: 113.4709 - val_mae: 113.4709\n",
      "Epoch 6/500\n",
      "510300/510300 [==============================] - 31s 61us/step - loss: 112.0261 - mae: 112.0261 - val_loss: 121.0198 - val_mae: 121.0198\n",
      "Epoch 7/500\n",
      "510300/510300 [==============================] - 29s 56us/step - loss: 99.4969 - mae: 99.4969 - val_loss: 125.5767 - val_mae: 125.5767\n",
      "Epoch 8/500\n",
      "510300/510300 [==============================] - 29s 57us/step - loss: 87.7326 - mae: 87.7326 - val_loss: 118.9464 - val_mae: 118.9464\n",
      "Epoch 9/500\n",
      "510300/510300 [==============================] - 29s 57us/step - loss: 78.4111 - mae: 78.4111 - val_loss: 101.3789 - val_mae: 101.3789\n",
      "Epoch 10/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 71.3867 - mae: 71.3867 - val_loss: 91.2489 - val_mae: 91.2489\n",
      "Epoch 11/500\n",
      "510300/510300 [==============================] - 26s 52us/step - loss: 66.0123 - mae: 66.0123 - val_loss: 86.9498 - val_mae: 86.9499\n",
      "Epoch 12/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 62.1486 - mae: 62.1486 - val_loss: 84.2900 - val_mae: 84.2900\n",
      "Epoch 13/500\n",
      "510300/510300 [==============================] - 29s 56us/step - loss: 59.2300 - mae: 59.2300 - val_loss: 77.8028 - val_mae: 77.8028\n",
      "Epoch 14/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 56.4806 - mae: 56.4806 - val_loss: 76.1307 - val_mae: 76.1307\n",
      "Epoch 15/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 53.6002 - mae: 53.6002 - val_loss: 70.0011 - val_mae: 70.0011\n",
      "Epoch 16/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 50.6349 - mae: 50.6349 - val_loss: 65.7822 - val_mae: 65.7822\n",
      "Epoch 17/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 47.6821 - mae: 47.6821 - val_loss: 60.3257 - val_mae: 60.3257\n",
      "Epoch 18/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 44.8698 - mae: 44.8698 - val_loss: 54.4638 - val_mae: 54.4638\n",
      "Epoch 19/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 42.1495 - mae: 42.1495 - val_loss: 50.7083 - val_mae: 50.7083\n",
      "Epoch 20/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 39.6506 - mae: 39.6507 - val_loss: 46.6299 - val_mae: 46.6299\n",
      "Epoch 21/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 37.3945 - mae: 37.3945 - val_loss: 43.3691 - val_mae: 43.3691\n",
      "Epoch 22/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 35.2857 - mae: 35.2857 - val_loss: 40.4013 - val_mae: 40.4013\n",
      "Epoch 23/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 33.4165 - mae: 33.4165 - val_loss: 38.2296 - val_mae: 38.2295\n",
      "Epoch 24/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 31.7411 - mae: 31.7411 - val_loss: 37.7111 - val_mae: 37.7111\n",
      "Epoch 25/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 30.2713 - mae: 30.2713 - val_loss: 34.6467 - val_mae: 34.6467\n",
      "Epoch 26/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 29.0061 - mae: 29.0061 - val_loss: 33.4584 - val_mae: 33.4584\n",
      "Epoch 27/500\n",
      "510300/510300 [==============================] - 26s 52us/step - loss: 27.8846 - mae: 27.8846 - val_loss: 31.3997 - val_mae: 31.3997\n",
      "Epoch 28/500\n",
      "510300/510300 [==============================] - 30s 59us/step - loss: 26.8930 - mae: 26.8930 - val_loss: 31.4483 - val_mae: 31.4483\n",
      "Epoch 29/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 26.2022 - mae: 26.2022 - val_loss: 30.2486 - val_mae: 30.2486\n",
      "Epoch 30/500\n",
      "510300/510300 [==============================] - 26s 52us/step - loss: 25.3228 - mae: 25.3228 - val_loss: 29.3265 - val_mae: 29.3265\n",
      "Epoch 31/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 24.7011 - mae: 24.7011 - val_loss: 28.5704 - val_mae: 28.5704\n",
      "Epoch 32/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 24.2313 - mae: 24.2313 - val_loss: 28.5264 - val_mae: 28.5264\n",
      "Epoch 33/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 23.5683 - mae: 23.5683 - val_loss: 26.6049 - val_mae: 26.6049\n",
      "Epoch 34/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 23.1058 - mae: 23.1058 - val_loss: 26.0661 - val_mae: 26.0661\n",
      "Epoch 35/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 22.7266 - mae: 22.7266 - val_loss: 25.7622 - val_mae: 25.7622\n",
      "Epoch 36/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 22.2782 - mae: 22.2782 - val_loss: 24.8680 - val_mae: 24.8680\n",
      "Epoch 37/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 21.9523 - mae: 21.9523 - val_loss: 23.9997 - val_mae: 23.9997\n",
      "Epoch 38/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 21.5984 - mae: 21.5984 - val_loss: 23.9771 - val_mae: 23.9771\n",
      "Epoch 39/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 21.2301 - mae: 21.2301 - val_loss: 23.4175 - val_mae: 23.4175\n",
      "Epoch 40/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 20.9825 - mae: 20.9825 - val_loss: 23.1244 - val_mae: 23.1244\n",
      "Epoch 41/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 20.7692 - mae: 20.7692 - val_loss: 22.4043 - val_mae: 22.4043\n",
      "Epoch 42/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 20.4378 - mae: 20.4378 - val_loss: 22.9002 - val_mae: 22.9002\n",
      "Epoch 43/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 20.2936 - mae: 20.2936 - val_loss: 21.8095 - val_mae: 21.8095\n",
      "Epoch 44/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 19.9541 - mae: 19.9541 - val_loss: 21.3317 - val_mae: 21.3317\n",
      "Epoch 45/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 19.7920 - mae: 19.7920 - val_loss: 21.7028 - val_mae: 21.7028\n",
      "Epoch 46/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 19.6111 - mae: 19.6111 - val_loss: 21.1250 - val_mae: 21.1250\n",
      "Epoch 47/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 19.3874 - mae: 19.3874 - val_loss: 20.9606 - val_mae: 20.9606\n",
      "Epoch 48/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 19.1236 - mae: 19.1236 - val_loss: 20.7287 - val_mae: 20.7287\n",
      "Epoch 49/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 18.9603 - mae: 18.9603 - val_loss: 20.9770 - val_mae: 20.9770\n",
      "Epoch 50/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 18.8063 - mae: 18.8063 - val_loss: 20.1760 - val_mae: 20.1761\n",
      "Epoch 51/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 18.7160 - mae: 18.7160 - val_loss: 20.3220 - val_mae: 20.3220\n",
      "Epoch 52/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 18.4752 - mae: 18.4752 - val_loss: 20.6407 - val_mae: 20.6407\n",
      "Epoch 53/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 18.4995 - mae: 18.4995 - val_loss: 19.6261 - val_mae: 19.6261\n",
      "Epoch 54/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 18.2061 - mae: 18.2061 - val_loss: 19.5023 - val_mae: 19.5023\n",
      "Epoch 55/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 18.1076 - mae: 18.1076 - val_loss: 19.4896 - val_mae: 19.4896\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510300/510300 [==============================] - 25s 49us/step - loss: 17.8696 - mae: 17.8696 - val_loss: 19.3536 - val_mae: 19.3536\n",
      "Epoch 57/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 17.8106 - mae: 17.8106 - val_loss: 19.0140 - val_mae: 19.0140\n",
      "Epoch 58/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 17.6103 - mae: 17.6103 - val_loss: 19.0181 - val_mae: 19.0181\n",
      "Epoch 59/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 17.5248 - mae: 17.5248 - val_loss: 18.9584 - val_mae: 18.9584\n",
      "Epoch 60/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 17.4297 - mae: 17.4297 - val_loss: 18.8791 - val_mae: 18.8791\n",
      "Epoch 61/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 17.2469 - mae: 17.2469 - val_loss: 18.1871 - val_mae: 18.1871\n",
      "Epoch 62/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 17.1503 - mae: 17.1503 - val_loss: 18.8141 - val_mae: 18.8141\n",
      "Epoch 63/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 17.0801 - mae: 17.0801 - val_loss: 18.6473 - val_mae: 18.6473\n",
      "Epoch 64/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.9978 - mae: 16.9978 - val_loss: 17.8370 - val_mae: 17.8370\n",
      "Epoch 65/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.8123 - mae: 16.8123 - val_loss: 17.7485 - val_mae: 17.7485\n",
      "Epoch 66/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.7020 - mae: 16.7020 - val_loss: 17.7442 - val_mae: 17.7442\n",
      "Epoch 67/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 16.6018 - mae: 16.6018 - val_loss: 18.2116 - val_mae: 18.2116\n",
      "Epoch 68/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 16.5884 - mae: 16.5884 - val_loss: 17.8717 - val_mae: 17.8717\n",
      "Epoch 69/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.4466 - mae: 16.4466 - val_loss: 18.0406 - val_mae: 18.0406\n",
      "Epoch 70/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.4366 - mae: 16.4366 - val_loss: 18.0627 - val_mae: 18.0627\n",
      "Epoch 71/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.3282 - mae: 16.3282 - val_loss: 18.3853 - val_mae: 18.3853\n",
      "Epoch 72/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 16.2998 - mae: 16.2998 - val_loss: 17.6217 - val_mae: 17.6217\n",
      "Epoch 73/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 16.1755 - mae: 16.1755 - val_loss: 18.2222 - val_mae: 18.2222\n",
      "Epoch 74/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 16.1346 - mae: 16.1346 - val_loss: 17.0320 - val_mae: 17.0320\n",
      "Epoch 75/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 15.9325 - mae: 15.9325 - val_loss: 17.0970 - val_mae: 17.0970\n",
      "Epoch 76/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.8798 - mae: 15.8798 - val_loss: 17.0277 - val_mae: 17.0277\n",
      "Epoch 77/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 15.7288 - mae: 15.7288 - val_loss: 17.4851 - val_mae: 17.4851\n",
      "Epoch 78/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.7519 - mae: 15.7519 - val_loss: 16.7807 - val_mae: 16.7807\n",
      "Epoch 79/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.6136 - mae: 15.6136 - val_loss: 16.9297 - val_mae: 16.9297\n",
      "Epoch 80/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.6031 - mae: 15.6031 - val_loss: 17.3047 - val_mae: 17.3047\n",
      "Epoch 81/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.5157 - mae: 15.5157 - val_loss: 16.3864 - val_mae: 16.3864\n",
      "Epoch 82/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.4022 - mae: 15.4022 - val_loss: 16.8827 - val_mae: 16.8827\n",
      "Epoch 83/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.3218 - mae: 15.3218 - val_loss: 16.9190 - val_mae: 16.9190\n",
      "Epoch 84/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.2981 - mae: 15.2980 - val_loss: 16.3504 - val_mae: 16.3504\n",
      "Epoch 85/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.1968 - mae: 15.1968 - val_loss: 16.8184 - val_mae: 16.8184\n",
      "Epoch 86/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 15.2176 - mae: 15.2176 - val_loss: 16.2436 - val_mae: 16.2436\n",
      "Epoch 87/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.0623 - mae: 15.0623 - val_loss: 16.2566 - val_mae: 16.2566\n",
      "Epoch 88/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 15.1097 - mae: 15.1097 - val_loss: 16.1783 - val_mae: 16.1783\n",
      "Epoch 89/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.9884 - mae: 14.9884 - val_loss: 15.9869 - val_mae: 15.9869\n",
      "Epoch 90/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.9148 - mae: 14.9148 - val_loss: 15.9054 - val_mae: 15.9054\n",
      "Epoch 91/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 14.7816 - mae: 14.7816 - val_loss: 16.1777 - val_mae: 16.1777\n",
      "Epoch 92/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 14.7889 - mae: 14.7889 - val_loss: 16.3418 - val_mae: 16.3418\n",
      "Epoch 93/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.7593 - mae: 14.7593 - val_loss: 16.2469 - val_mae: 16.2469\n",
      "Epoch 94/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 14.7638 - mae: 14.7638 - val_loss: 16.0358 - val_mae: 16.0358\n",
      "Epoch 95/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.6141 - mae: 14.6140 - val_loss: 15.6122 - val_mae: 15.6122\n",
      "Epoch 96/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 14.5333 - mae: 14.5333 - val_loss: 15.5818 - val_mae: 15.5818\n",
      "Epoch 97/500\n",
      "510300/510300 [==============================] - 27s 54us/step - loss: 14.5230 - mae: 14.5230 - val_loss: 16.0087 - val_mae: 16.0087\n",
      "Epoch 98/500\n",
      "510300/510300 [==============================] - 29s 57us/step - loss: 14.5208 - mae: 14.5208 - val_loss: 16.5239 - val_mae: 16.5239\n",
      "Epoch 99/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 14.5458 - mae: 14.5458 - val_loss: 15.8731 - val_mae: 15.8731\n",
      "Epoch 100/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 14.3382 - mae: 14.3382 - val_loss: 15.8313 - val_mae: 15.8313\n",
      "Epoch 101/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.3127 - mae: 14.3127 - val_loss: 15.4422 - val_mae: 15.4422\n",
      "Epoch 102/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.2714 - mae: 14.2714 - val_loss: 15.2250 - val_mae: 15.2250\n",
      "Epoch 103/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.1910 - mae: 14.1910 - val_loss: 15.3751 - val_mae: 15.3751\n",
      "Epoch 104/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 14.1067 - mae: 14.1067 - val_loss: 15.4991 - val_mae: 15.4991\n",
      "Epoch 105/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 14.1210 - mae: 14.1210 - val_loss: 15.2218 - val_mae: 15.2218\n",
      "Epoch 106/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 14.0358 - mae: 14.0358 - val_loss: 15.2147 - val_mae: 15.2147\n",
      "Epoch 107/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 14.0380 - mae: 14.0380 - val_loss: 15.1686 - val_mae: 15.1686\n",
      "Epoch 108/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.9625 - mae: 13.9625 - val_loss: 15.3550 - val_mae: 15.3550\n",
      "Epoch 109/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.9358 - mae: 13.9358 - val_loss: 15.1253 - val_mae: 15.1253\n",
      "Epoch 110/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 13.9212 - mae: 13.9212 - val_loss: 14.7219 - val_mae: 14.7219\n",
      "Epoch 111/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.8172 - mae: 13.8172 - val_loss: 15.4844 - val_mae: 15.4844\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.8263 - mae: 13.8263 - val_loss: 15.0423 - val_mae: 15.0423\n",
      "Epoch 113/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.7616 - mae: 13.7616 - val_loss: 15.0596 - val_mae: 15.0596\n",
      "Epoch 114/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.7629 - mae: 13.7629 - val_loss: 15.1422 - val_mae: 15.1422\n",
      "Epoch 115/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.7135 - mae: 13.7135 - val_loss: 14.7509 - val_mae: 14.7509\n",
      "Epoch 116/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.6137 - mae: 13.6137 - val_loss: 14.9623 - val_mae: 14.9623\n",
      "Epoch 117/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.5955 - mae: 13.5955 - val_loss: 14.8377 - val_mae: 14.8377\n",
      "Epoch 118/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.6224 - mae: 13.6224 - val_loss: 14.8673 - val_mae: 14.8673\n",
      "Epoch 119/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.4943 - mae: 13.4943 - val_loss: 14.6684 - val_mae: 14.6684\n",
      "Epoch 120/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.4724 - mae: 13.4724 - val_loss: 14.5464 - val_mae: 14.5464\n",
      "Epoch 121/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.4313 - mae: 13.4313 - val_loss: 14.4790 - val_mae: 14.4790\n",
      "Epoch 122/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.4346 - mae: 13.4346 - val_loss: 15.0178 - val_mae: 15.0178\n",
      "Epoch 123/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.3491 - mae: 13.3491 - val_loss: 14.6453 - val_mae: 14.6453\n",
      "Epoch 124/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.2808 - mae: 13.2808 - val_loss: 14.4403 - val_mae: 14.4403\n",
      "Epoch 125/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.2313 - mae: 13.2313 - val_loss: 14.8720 - val_mae: 14.8720\n",
      "Epoch 126/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.2210 - mae: 13.2210 - val_loss: 14.7947 - val_mae: 14.7947\n",
      "Epoch 127/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.2749 - mae: 13.2749 - val_loss: 14.3171 - val_mae: 14.3171\n",
      "Epoch 128/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.1406 - mae: 13.1406 - val_loss: 14.2217 - val_mae: 14.2217\n",
      "Epoch 129/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 13.1412 - mae: 13.1412 - val_loss: 14.7617 - val_mae: 14.7617\n",
      "Epoch 130/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.0918 - mae: 13.0918 - val_loss: 14.6008 - val_mae: 14.6008\n",
      "Epoch 131/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 13.0217 - mae: 13.0217 - val_loss: 13.9805 - val_mae: 13.9805\n",
      "Epoch 132/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.9347 - mae: 12.9347 - val_loss: 14.3427 - val_mae: 14.3427\n",
      "Epoch 133/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.9669 - mae: 12.9669 - val_loss: 14.2613 - val_mae: 14.2613\n",
      "Epoch 134/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 12.9205 - mae: 12.9205 - val_loss: 14.1731 - val_mae: 14.1731\n",
      "Epoch 135/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.9246 - mae: 12.9246 - val_loss: 14.2508 - val_mae: 14.2508\n",
      "Epoch 136/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.8801 - mae: 12.8801 - val_loss: 14.2018 - val_mae: 14.2018\n",
      "Epoch 137/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.8704 - mae: 12.8704 - val_loss: 13.9314 - val_mae: 13.9314\n",
      "Epoch 138/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 12.7527 - mae: 12.7527 - val_loss: 13.9049 - val_mae: 13.9049\n",
      "Epoch 139/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 12.8232 - mae: 12.8232 - val_loss: 14.1064 - val_mae: 14.1064\n",
      "Epoch 140/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.7052 - mae: 12.7052 - val_loss: 13.6765 - val_mae: 13.6765\n",
      "Epoch 141/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.6360 - mae: 12.6360 - val_loss: 13.8742 - val_mae: 13.8742\n",
      "Epoch 142/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.6489 - mae: 12.6489 - val_loss: 13.8677 - val_mae: 13.8677\n",
      "Epoch 143/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.6076 - mae: 12.6076 - val_loss: 14.4580 - val_mae: 14.4580\n",
      "Epoch 144/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 12.7755 - mae: 12.7755 - val_loss: 13.4975 - val_mae: 13.4975\n",
      "Epoch 145/500\n",
      "510300/510300 [==============================] - 31s 61us/step - loss: 12.5001 - mae: 12.5001 - val_loss: 13.5423 - val_mae: 13.5423\n",
      "Epoch 146/500\n",
      "510300/510300 [==============================] - 28s 56us/step - loss: 12.5063 - mae: 12.5063 - val_loss: 13.3837 - val_mae: 13.3837\n",
      "Epoch 147/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.4963 - mae: 12.4963 - val_loss: 13.5911 - val_mae: 13.5911\n",
      "Epoch 148/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.4494 - mae: 12.4494 - val_loss: 13.3290 - val_mae: 13.3290\n",
      "Epoch 149/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.4408 - mae: 12.4408 - val_loss: 13.6253 - val_mae: 13.6253\n",
      "Epoch 150/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.4413 - mae: 12.4413 - val_loss: 13.4159 - val_mae: 13.4159\n",
      "Epoch 151/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.3866 - mae: 12.3866 - val_loss: 13.7332 - val_mae: 13.7332\n",
      "Epoch 152/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.4205 - mae: 12.4205 - val_loss: 13.3541 - val_mae: 13.3541\n",
      "Epoch 153/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.3394 - mae: 12.3394 - val_loss: 13.8562 - val_mae: 13.8562\n",
      "Epoch 154/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.4007 - mae: 12.4007 - val_loss: 13.6728 - val_mae: 13.6728\n",
      "Epoch 155/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.2880 - mae: 12.2880 - val_loss: 13.4898 - val_mae: 13.4898\n",
      "Epoch 156/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.2454 - mae: 12.2454 - val_loss: 13.3195 - val_mae: 13.3195\n",
      "Epoch 157/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.2650 - mae: 12.2650 - val_loss: 13.4316 - val_mae: 13.4316\n",
      "Epoch 158/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.2169 - mae: 12.2169 - val_loss: 13.6524 - val_mae: 13.6524\n",
      "Epoch 159/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.2571 - mae: 12.2571 - val_loss: 13.1633 - val_mae: 13.1633\n",
      "Epoch 160/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.1235 - mae: 12.1235 - val_loss: 13.1796 - val_mae: 13.1796\n",
      "Epoch 161/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.1133 - mae: 12.1133 - val_loss: 13.0481 - val_mae: 13.0481\n",
      "Epoch 162/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.0734 - mae: 12.0734 - val_loss: 13.6010 - val_mae: 13.6010\n",
      "Epoch 163/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 12.1090 - mae: 12.1090 - val_loss: 13.0784 - val_mae: 13.0784\n",
      "Epoch 164/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 12.0080 - mae: 12.0080 - val_loss: 12.9301 - val_mae: 12.9301\n",
      "Epoch 165/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.9481 - mae: 11.9481 - val_loss: 13.0616 - val_mae: 13.0616\n",
      "Epoch 166/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.9644 - mae: 11.9644 - val_loss: 13.3085 - val_mae: 13.3085\n",
      "Epoch 167/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 12.0454 - mae: 12.0454 - val_loss: 13.4010 - val_mae: 13.4010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 12.0111 - mae: 12.0111 - val_loss: 13.3477 - val_mae: 13.3477\n",
      "Epoch 169/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.9615 - mae: 11.9615 - val_loss: 13.0942 - val_mae: 13.0942\n",
      "Epoch 170/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.8628 - mae: 11.8628 - val_loss: 13.1181 - val_mae: 13.1181\n",
      "Epoch 171/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.8508 - mae: 11.8508 - val_loss: 12.7127 - val_mae: 12.7127\n",
      "Epoch 172/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.8127 - mae: 11.8127 - val_loss: 13.1953 - val_mae: 13.1953\n",
      "Epoch 173/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 11.9288 - mae: 11.9288 - val_loss: 13.0326 - val_mae: 13.0326\n",
      "Epoch 174/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 11.8370 - mae: 11.8370 - val_loss: 12.8677 - val_mae: 12.8677\n",
      "Epoch 175/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.8087 - mae: 11.8087 - val_loss: 12.9183 - val_mae: 12.9183\n",
      "Epoch 176/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 11.7210 - mae: 11.7210 - val_loss: 12.9302 - val_mae: 12.9302\n",
      "Epoch 177/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.6943 - mae: 11.6943 - val_loss: 12.9707 - val_mae: 12.9707\n",
      "Epoch 178/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.6707 - mae: 11.6707 - val_loss: 12.9255 - val_mae: 12.9255\n",
      "Epoch 179/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.7041 - mae: 11.7041 - val_loss: 12.6341 - val_mae: 12.6341\n",
      "Epoch 180/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.6286 - mae: 11.6286 - val_loss: 12.6556 - val_mae: 12.6557\n",
      "Epoch 181/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 11.6722 - mae: 11.6722 - val_loss: 12.8547 - val_mae: 12.8547\n",
      "Epoch 182/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.5784 - mae: 11.5784 - val_loss: 12.4585 - val_mae: 12.4585\n",
      "Epoch 183/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.5223 - mae: 11.5223 - val_loss: 12.5818 - val_mae: 12.5818\n",
      "Epoch 184/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.5464 - mae: 11.5464 - val_loss: 12.5921 - val_mae: 12.5921\n",
      "Epoch 185/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.5532 - mae: 11.5532 - val_loss: 12.4971 - val_mae: 12.4971\n",
      "Epoch 186/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.4549 - mae: 11.4549 - val_loss: 12.6503 - val_mae: 12.6503\n",
      "Epoch 187/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.5595 - mae: 11.5595 - val_loss: 13.1346 - val_mae: 13.1346\n",
      "Epoch 188/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.6412 - mae: 11.6412 - val_loss: 12.6387 - val_mae: 12.6387\n",
      "Epoch 189/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.4357 - mae: 11.4357 - val_loss: 12.9452 - val_mae: 12.9452\n",
      "Epoch 190/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.4841 - mae: 11.4841 - val_loss: 13.1026 - val_mae: 13.1026\n",
      "Epoch 191/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.4464 - mae: 11.4464 - val_loss: 12.5904 - val_mae: 12.5904\n",
      "Epoch 192/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.4448 - mae: 11.4448 - val_loss: 13.4321 - val_mae: 13.4321\n",
      "Epoch 193/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.5622 - mae: 11.5622 - val_loss: 13.2035 - val_mae: 13.2035\n",
      "Epoch 194/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.4096 - mae: 11.4096 - val_loss: 12.3821 - val_mae: 12.3821\n",
      "Epoch 195/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.2461 - mae: 11.2461 - val_loss: 12.7412 - val_mae: 12.7412\n",
      "Epoch 196/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.3239 - mae: 11.3239 - val_loss: 12.3428 - val_mae: 12.3428\n",
      "Epoch 197/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.3543 - mae: 11.3543 - val_loss: 12.7180 - val_mae: 12.7180\n",
      "Epoch 198/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.3412 - mae: 11.3412 - val_loss: 12.4455 - val_mae: 12.4455\n",
      "Epoch 199/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.2527 - mae: 11.2527 - val_loss: 12.6142 - val_mae: 12.6142\n",
      "Epoch 200/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.3292 - mae: 11.3292 - val_loss: 11.9992 - val_mae: 11.9992\n",
      "Epoch 201/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.1830 - mae: 11.1830 - val_loss: 12.2597 - val_mae: 12.2597\n",
      "Epoch 202/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.2096 - mae: 11.2096 - val_loss: 12.1384 - val_mae: 12.1384\n",
      "Epoch 203/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.1293 - mae: 11.1293 - val_loss: 12.4490 - val_mae: 12.4490\n",
      "Epoch 204/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.1879 - mae: 11.1879 - val_loss: 12.3010 - val_mae: 12.3010\n",
      "Epoch 205/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 11.1363 - mae: 11.1363 - val_loss: 12.2810 - val_mae: 12.2810\n",
      "Epoch 206/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 11.0578 - mae: 11.0578 - val_loss: 12.4207 - val_mae: 12.4207\n",
      "Epoch 207/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.1828 - mae: 11.1828 - val_loss: 12.0520 - val_mae: 12.0520\n",
      "Epoch 208/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.0450 - mae: 11.0450 - val_loss: 12.2170 - val_mae: 12.2170\n",
      "Epoch 209/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.0537 - mae: 11.0537 - val_loss: 11.9730 - val_mae: 11.9730\n",
      "Epoch 210/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.9902 - mae: 10.9902 - val_loss: 12.5536 - val_mae: 12.5536\n",
      "Epoch 211/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.0932 - mae: 11.0932 - val_loss: 12.2220 - val_mae: 12.2220\n",
      "Epoch 212/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.0095 - mae: 11.0095 - val_loss: 12.0241 - val_mae: 12.0241\n",
      "Epoch 213/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.0128 - mae: 11.0128 - val_loss: 12.1342 - val_mae: 12.1342\n",
      "Epoch 214/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.9429 - mae: 10.9429 - val_loss: 12.2162 - val_mae: 12.2162\n",
      "Epoch 215/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.9961 - mae: 10.9961 - val_loss: 12.3589 - val_mae: 12.3589\n",
      "Epoch 216/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 11.0033 - mae: 11.0033 - val_loss: 11.9706 - val_mae: 11.9706\n",
      "Epoch 217/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.8830 - mae: 10.8830 - val_loss: 12.5250 - val_mae: 12.5250\n",
      "Epoch 218/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.9957 - mae: 10.9957 - val_loss: 12.1972 - val_mae: 12.1972\n",
      "Epoch 219/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.8443 - mae: 10.8443 - val_loss: 12.2107 - val_mae: 12.2107\n",
      "Epoch 220/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.8694 - mae: 10.8694 - val_loss: 11.9229 - val_mae: 11.9229\n",
      "Epoch 221/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.8306 - mae: 10.8306 - val_loss: 12.3075 - val_mae: 12.3075\n",
      "Epoch 222/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 10.9062 - mae: 10.9062 - val_loss: 11.8775 - val_mae: 11.8775\n",
      "Epoch 223/500\n",
      "510300/510300 [==============================] - 26s 52us/step - loss: 10.7633 - mae: 10.7633 - val_loss: 12.2900 - val_mae: 12.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.7810 - mae: 10.7810 - val_loss: 11.9731 - val_mae: 11.9731\n",
      "Epoch 225/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.8350 - mae: 10.8350 - val_loss: 11.7641 - val_mae: 11.7641\n",
      "Epoch 226/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.7459 - mae: 10.7459 - val_loss: 12.2571 - val_mae: 12.2571\n",
      "Epoch 227/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.8275 - mae: 10.8275 - val_loss: 12.1486 - val_mae: 12.1486\n",
      "Epoch 228/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.8600 - mae: 10.8600 - val_loss: 12.0559 - val_mae: 12.0559\n",
      "Epoch 229/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.7504 - mae: 10.7504 - val_loss: 11.7902 - val_mae: 11.7902\n",
      "Epoch 230/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.7494 - mae: 10.7494 - val_loss: 11.7703 - val_mae: 11.7703\n",
      "Epoch 231/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.7084 - mae: 10.7084 - val_loss: 11.9501 - val_mae: 11.9501\n",
      "Epoch 232/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.7517 - mae: 10.7517 - val_loss: 11.6272 - val_mae: 11.6272\n",
      "Epoch 233/500\n",
      "510300/510300 [==============================] - 27s 54us/step - loss: 10.6323 - mae: 10.6323 - val_loss: 11.8316 - val_mae: 11.8316\n",
      "Epoch 234/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.6552 - mae: 10.6552 - val_loss: 11.7629 - val_mae: 11.7629\n",
      "Epoch 235/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.6823 - mae: 10.6823 - val_loss: 11.6600 - val_mae: 11.6600\n",
      "Epoch 236/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 10.6239 - mae: 10.6239 - val_loss: 11.9813 - val_mae: 11.9813\n",
      "Epoch 237/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.6337 - mae: 10.6337 - val_loss: 11.7990 - val_mae: 11.7990\n",
      "Epoch 238/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.6146 - mae: 10.6146 - val_loss: 11.8126 - val_mae: 11.8126\n",
      "Epoch 239/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.5697 - mae: 10.5697 - val_loss: 12.0489 - val_mae: 12.0489\n",
      "Epoch 240/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.6123 - mae: 10.6123 - val_loss: 11.5935 - val_mae: 11.5935\n",
      "Epoch 241/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.5247 - mae: 10.5247 - val_loss: 11.4973 - val_mae: 11.4973\n",
      "Epoch 242/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.5195 - mae: 10.5195 - val_loss: 12.0814 - val_mae: 12.0814\n",
      "Epoch 243/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.5336 - mae: 10.5336 - val_loss: 11.9607 - val_mae: 11.9607\n",
      "Epoch 244/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.4914 - mae: 10.4914 - val_loss: 11.8774 - val_mae: 11.8774\n",
      "Epoch 245/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.5621 - mae: 10.5621 - val_loss: 11.6467 - val_mae: 11.6467\n",
      "Epoch 246/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.5273 - mae: 10.5273 - val_loss: 11.6552 - val_mae: 11.6552\n",
      "Epoch 247/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.4801 - mae: 10.4801 - val_loss: 11.4977 - val_mae: 11.4977\n",
      "Epoch 248/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.4457 - mae: 10.4457 - val_loss: 11.6660 - val_mae: 11.6660\n",
      "Epoch 249/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.4959 - mae: 10.4959 - val_loss: 11.6409 - val_mae: 11.6409\n",
      "Epoch 250/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.3845 - mae: 10.3845 - val_loss: 11.6651 - val_mae: 11.6651\n",
      "Epoch 251/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.4376 - mae: 10.4376 - val_loss: 11.4050 - val_mae: 11.4050\n",
      "Epoch 252/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.3387 - mae: 10.3387 - val_loss: 11.4130 - val_mae: 11.4130\n",
      "Epoch 253/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.3664 - mae: 10.3664 - val_loss: 11.6358 - val_mae: 11.6358\n",
      "Epoch 254/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.3428 - mae: 10.3428 - val_loss: 11.8225 - val_mae: 11.8225\n",
      "Epoch 255/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.4639 - mae: 10.4639 - val_loss: 11.7267 - val_mae: 11.7267\n",
      "Epoch 256/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.3437 - mae: 10.3437 - val_loss: 11.4609 - val_mae: 11.4609\n",
      "Epoch 257/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.3641 - mae: 10.3641 - val_loss: 11.3083 - val_mae: 11.3083\n",
      "Epoch 258/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2827 - mae: 10.2827 - val_loss: 11.6723 - val_mae: 11.6723\n",
      "Epoch 259/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.2917 - mae: 10.2917 - val_loss: 11.4363 - val_mae: 11.4363\n",
      "Epoch 260/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2736 - mae: 10.2736 - val_loss: 11.3186 - val_mae: 11.3186\n",
      "Epoch 261/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2886 - mae: 10.2886 - val_loss: 11.6359 - val_mae: 11.6359\n",
      "Epoch 262/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2387 - mae: 10.2387 - val_loss: 11.2786 - val_mae: 11.2786\n",
      "Epoch 263/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2351 - mae: 10.2351 - val_loss: 12.0652 - val_mae: 12.0652\n",
      "Epoch 264/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.4096 - mae: 10.4096 - val_loss: 11.5576 - val_mae: 11.5576\n",
      "Epoch 265/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2613 - mae: 10.2613 - val_loss: 11.3605 - val_mae: 11.3605\n",
      "Epoch 266/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.2824 - mae: 10.2824 - val_loss: 11.2402 - val_mae: 11.2402\n",
      "Epoch 267/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.1737 - mae: 10.1737 - val_loss: 11.3540 - val_mae: 11.3540\n",
      "Epoch 268/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 10.1588 - mae: 10.1588 - val_loss: 11.1943 - val_mae: 11.1943\n",
      "Epoch 269/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 10.1686 - mae: 10.1686 - val_loss: 11.2769 - val_mae: 11.2769\n",
      "Epoch 270/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.1528 - mae: 10.1528 - val_loss: 11.3368 - val_mae: 11.3368\n",
      "Epoch 271/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.1487 - mae: 10.1487 - val_loss: 11.8100 - val_mae: 11.8101\n",
      "Epoch 272/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.2932 - mae: 10.2932 - val_loss: 11.2961 - val_mae: 11.2961\n",
      "Epoch 273/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.1516 - mae: 10.1516 - val_loss: 11.4817 - val_mae: 11.4817\n",
      "Epoch 274/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.1352 - mae: 10.1352 - val_loss: 11.2531 - val_mae: 11.2531\n",
      "Epoch 275/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.1711 - mae: 10.1711 - val_loss: 11.4939 - val_mae: 11.4939\n",
      "Epoch 276/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.1620 - mae: 10.1620 - val_loss: 11.2531 - val_mae: 11.2531\n",
      "Epoch 277/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.1416 - mae: 10.1416 - val_loss: 11.3951 - val_mae: 11.3951\n",
      "Epoch 278/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 10.0824 - mae: 10.0824 - val_loss: 11.9905 - val_mae: 11.9905\n",
      "Epoch 279/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 10.1630 - mae: 10.1630 - val_loss: 10.9440 - val_mae: 10.9440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.9769 - mae: 9.9769 - val_loss: 11.4299 - val_mae: 11.4299\n",
      "Epoch 281/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.0718 - mae: 10.0718 - val_loss: 11.1346 - val_mae: 11.1346\n",
      "Epoch 282/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 10.0303 - mae: 10.0303 - val_loss: 11.2130 - val_mae: 11.2130\n",
      "Epoch 283/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.9467 - mae: 9.9467 - val_loss: 11.3661 - val_mae: 11.3661\n",
      "Epoch 284/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.9579 - mae: 9.9579 - val_loss: 11.0758 - val_mae: 11.0758\n",
      "Epoch 285/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.9962 - mae: 9.9962 - val_loss: 11.0985 - val_mae: 11.0985\n",
      "Epoch 286/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.9874 - mae: 9.9874 - val_loss: 10.9368 - val_mae: 10.9368\n",
      "Epoch 287/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.9025 - mae: 9.9025 - val_loss: 11.1086 - val_mae: 11.1086\n",
      "Epoch 288/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.9795 - mae: 9.9795 - val_loss: 11.2538 - val_mae: 11.2538\n",
      "Epoch 289/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.9482 - mae: 9.9482 - val_loss: 10.8868 - val_mae: 10.8868\n",
      "Epoch 290/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8311 - mae: 9.8311 - val_loss: 11.1623 - val_mae: 11.1623\n",
      "Epoch 291/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8986 - mae: 9.8986 - val_loss: 11.1139 - val_mae: 11.1139\n",
      "Epoch 292/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8999 - mae: 9.8999 - val_loss: 10.8549 - val_mae: 10.8549\n",
      "Epoch 293/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.9176 - mae: 9.9176 - val_loss: 11.0710 - val_mae: 11.0710\n",
      "Epoch 294/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8642 - mae: 9.8642 - val_loss: 11.0633 - val_mae: 11.0633\n",
      "Epoch 295/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8435 - mae: 9.8435 - val_loss: 11.0754 - val_mae: 11.0754\n",
      "Epoch 296/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8751 - mae: 9.8751 - val_loss: 11.1964 - val_mae: 11.1964\n",
      "Epoch 297/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8626 - mae: 9.8626 - val_loss: 11.2145 - val_mae: 11.2145\n",
      "Epoch 298/500\n",
      "510300/510300 [==============================] - 29s 56us/step - loss: 9.8137 - mae: 9.8137 - val_loss: 10.7885 - val_mae: 10.7885\n",
      "Epoch 299/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.8037 - mae: 9.8037 - val_loss: 11.0409 - val_mae: 11.0409\n",
      "Epoch 300/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.8140 - mae: 9.8140 - val_loss: 11.1362 - val_mae: 11.1362\n",
      "Epoch 301/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.9234 - mae: 9.9234 - val_loss: 10.9602 - val_mae: 10.9602\n",
      "Epoch 302/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.7372 - mae: 9.7372 - val_loss: 11.2489 - val_mae: 11.2489\n",
      "Epoch 303/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.8207 - mae: 9.8207 - val_loss: 11.0426 - val_mae: 11.0426\n",
      "Epoch 304/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.7527 - mae: 9.7527 - val_loss: 10.9229 - val_mae: 10.9229\n",
      "Epoch 305/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.7462 - mae: 9.7462 - val_loss: 10.9023 - val_mae: 10.9023\n",
      "Epoch 306/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6678 - mae: 9.6678 - val_loss: 10.7977 - val_mae: 10.7977\n",
      "Epoch 307/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.7837 - mae: 9.7837 - val_loss: 11.2183 - val_mae: 11.2183\n",
      "Epoch 308/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.7534 - mae: 9.7534 - val_loss: 10.8905 - val_mae: 10.8905\n",
      "Epoch 309/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.7149 - mae: 9.7149 - val_loss: 10.8189 - val_mae: 10.8189\n",
      "Epoch 310/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6974 - mae: 9.6974 - val_loss: 10.8861 - val_mae: 10.8861\n",
      "Epoch 311/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6668 - mae: 9.6668 - val_loss: 10.7141 - val_mae: 10.7141\n",
      "Epoch 312/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.6860 - mae: 9.6860 - val_loss: 10.7434 - val_mae: 10.7434\n",
      "Epoch 313/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6810 - mae: 9.6810 - val_loss: 10.7483 - val_mae: 10.7483\n",
      "Epoch 314/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.7078 - mae: 9.7078 - val_loss: 10.9856 - val_mae: 10.9856\n",
      "Epoch 315/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.5816 - mae: 9.5816 - val_loss: 10.7218 - val_mae: 10.7218\n",
      "Epoch 316/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6589 - mae: 9.6589 - val_loss: 10.9103 - val_mae: 10.9103\n",
      "Epoch 317/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.6721 - mae: 9.6721 - val_loss: 10.7308 - val_mae: 10.7308\n",
      "Epoch 318/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6392 - mae: 9.6392 - val_loss: 10.9075 - val_mae: 10.9075\n",
      "Epoch 319/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.7016 - mae: 9.7016 - val_loss: 10.8080 - val_mae: 10.8080\n",
      "Epoch 320/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.6941 - mae: 9.6941 - val_loss: 10.8253 - val_mae: 10.8253\n",
      "Epoch 321/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.6946 - mae: 9.6946 - val_loss: 10.8261 - val_mae: 10.8261\n",
      "Epoch 322/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 9.5996 - mae: 9.5996 - val_loss: 10.7366 - val_mae: 10.7366\n",
      "Epoch 323/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.6157 - mae: 9.6157 - val_loss: 10.8528 - val_mae: 10.8528\n",
      "Epoch 324/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.5705 - mae: 9.5705 - val_loss: 10.6806 - val_mae: 10.6806\n",
      "Epoch 325/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.5313 - mae: 9.5313 - val_loss: 10.7713 - val_mae: 10.7713\n",
      "Epoch 326/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.5972 - mae: 9.5972 - val_loss: 10.6906 - val_mae: 10.6906\n",
      "Epoch 327/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.5383 - mae: 9.5383 - val_loss: 10.5679 - val_mae: 10.5679\n",
      "Epoch 328/500\n",
      "510300/510300 [==============================] - 24s 48us/step - loss: 9.5805 - mae: 9.5805 - val_loss: 10.6135 - val_mae: 10.6135\n",
      "Epoch 329/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.4764 - mae: 9.4764 - val_loss: 10.6611 - val_mae: 10.6611\n",
      "Epoch 330/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4430 - mae: 9.4430 - val_loss: 10.9430 - val_mae: 10.9430\n",
      "Epoch 331/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.5553 - mae: 9.5553 - val_loss: 10.6883 - val_mae: 10.6883\n",
      "Epoch 332/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4888 - mae: 9.4889 - val_loss: 10.8746 - val_mae: 10.8746\n",
      "Epoch 333/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.5122 - mae: 9.5122 - val_loss: 10.8067 - val_mae: 10.8067\n",
      "Epoch 334/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.4739 - mae: 9.4739 - val_loss: 10.6606 - val_mae: 10.6606\n",
      "Epoch 335/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4701 - mae: 9.4701 - val_loss: 10.9265 - val_mae: 10.9265\n",
      "Epoch 336/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.5119 - mae: 9.5119 - val_loss: 10.3409 - val_mae: 10.3409\n",
      "Epoch 337/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.4164 - mae: 9.4164 - val_loss: 10.6345 - val_mae: 10.6345\n",
      "Epoch 338/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4329 - mae: 9.4329 - val_loss: 10.4320 - val_mae: 10.4320\n",
      "Epoch 339/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4265 - mae: 9.4265 - val_loss: 10.8164 - val_mae: 10.8164\n",
      "Epoch 340/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4469 - mae: 9.4469 - val_loss: 10.7138 - val_mae: 10.7138\n",
      "Epoch 341/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.4129 - mae: 9.4129 - val_loss: 10.5945 - val_mae: 10.5945\n",
      "Epoch 342/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.3318 - mae: 9.3318 - val_loss: 10.3018 - val_mae: 10.3018\n",
      "Epoch 343/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.3739 - mae: 9.3739 - val_loss: 10.6053 - val_mae: 10.6053\n",
      "Epoch 344/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.4611 - mae: 9.4611 - val_loss: 10.7817 - val_mae: 10.7817\n",
      "Epoch 345/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.3577 - mae: 9.3577 - val_loss: 10.5510 - val_mae: 10.5510\n",
      "Epoch 346/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.3417 - mae: 9.3417 - val_loss: 10.3812 - val_mae: 10.3812\n",
      "Epoch 347/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.3014 - mae: 9.3014 - val_loss: 10.4881 - val_mae: 10.4881\n",
      "Epoch 348/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.3418 - mae: 9.3418 - val_loss: 10.4396 - val_mae: 10.4396\n",
      "Epoch 349/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.3926 - mae: 9.3926 - val_loss: 10.2952 - val_mae: 10.2952\n",
      "Epoch 350/500\n",
      "510300/510300 [==============================] - 37s 73us/step - loss: 9.3076 - mae: 9.3076 - val_loss: 10.3634 - val_mae: 10.3634\n",
      "Epoch 351/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 9.3446 - mae: 9.3446 - val_loss: 10.9775 - val_mae: 10.9775\n",
      "Epoch 352/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.4532 - mae: 9.4532 - val_loss: 10.4492 - val_mae: 10.4492\n",
      "Epoch 353/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.3286 - mae: 9.3286 - val_loss: 10.4670 - val_mae: 10.4670\n",
      "Epoch 354/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.2944 - mae: 9.2944 - val_loss: 10.4454 - val_mae: 10.4454\n",
      "Epoch 355/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.3837 - mae: 9.3837 - val_loss: 10.6677 - val_mae: 10.6677\n",
      "Epoch 356/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.3599 - mae: 9.3599 - val_loss: 10.6505 - val_mae: 10.6505\n",
      "Epoch 357/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.3533 - mae: 9.3533 - val_loss: 10.3843 - val_mae: 10.3843\n",
      "Epoch 358/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.3302 - mae: 9.3302 - val_loss: 10.3452 - val_mae: 10.3452\n",
      "Epoch 359/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.2024 - mae: 9.2024 - val_loss: 10.3464 - val_mae: 10.3464\n",
      "Epoch 360/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.2061 - mae: 9.2061 - val_loss: 10.5577 - val_mae: 10.5577\n",
      "Epoch 361/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.2863 - mae: 9.2863 - val_loss: 10.3791 - val_mae: 10.3791\n",
      "Epoch 362/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1838 - mae: 9.1838 - val_loss: 10.7246 - val_mae: 10.7246\n",
      "Epoch 363/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.2529 - mae: 9.2529 - val_loss: 10.6030 - val_mae: 10.6030\n",
      "Epoch 364/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.2240 - mae: 9.2240 - val_loss: 10.3652 - val_mae: 10.3652\n",
      "Epoch 365/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1304 - mae: 9.1304 - val_loss: 10.2300 - val_mae: 10.2300\n",
      "Epoch 366/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1888 - mae: 9.1888 - val_loss: 10.2178 - val_mae: 10.2178\n",
      "Epoch 367/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1698 - mae: 9.1698 - val_loss: 10.2031 - val_mae: 10.2031\n",
      "Epoch 368/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.1545 - mae: 9.1545 - val_loss: 10.4281 - val_mae: 10.4281\n",
      "Epoch 369/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.1913 - mae: 9.1913 - val_loss: 10.2189 - val_mae: 10.2189\n",
      "Epoch 370/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1543 - mae: 9.1543 - val_loss: 10.5514 - val_mae: 10.5514\n",
      "Epoch 371/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.2700 - mae: 9.2700 - val_loss: 10.2141 - val_mae: 10.2141\n",
      "Epoch 372/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.1605 - mae: 9.1605 - val_loss: 10.3301 - val_mae: 10.3301\n",
      "Epoch 373/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1924 - mae: 9.1924 - val_loss: 10.4113 - val_mae: 10.4113\n",
      "Epoch 374/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.0837 - mae: 9.0837 - val_loss: 10.0104 - val_mae: 10.0104\n",
      "Epoch 375/500\n",
      "510300/510300 [==============================] - 35s 69us/step - loss: 9.0644 - mae: 9.0644 - val_loss: 10.7697 - val_mae: 10.7697\n",
      "Epoch 376/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 9.2132 - mae: 9.2132 - val_loss: 10.2701 - val_mae: 10.2701\n",
      "Epoch 377/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.1746 - mae: 9.1746 - val_loss: 10.2137 - val_mae: 10.2137\n",
      "Epoch 378/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.2050 - mae: 9.2050 - val_loss: 10.2100 - val_mae: 10.2100\n",
      "Epoch 379/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.0979 - mae: 9.0979 - val_loss: 10.0945 - val_mae: 10.0945\n",
      "Epoch 380/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.0884 - mae: 9.0884 - val_loss: 10.2309 - val_mae: 10.2309\n",
      "Epoch 381/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.1163 - mae: 9.1163 - val_loss: 10.1022 - val_mae: 10.1022\n",
      "Epoch 382/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 9.0249 - mae: 9.0249 - val_loss: 10.5768 - val_mae: 10.5768\n",
      "Epoch 383/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 9.0810 - mae: 9.0810 - val_loss: 10.4424 - val_mae: 10.4424\n",
      "Epoch 384/500\n",
      "510300/510300 [==============================] - 27s 54us/step - loss: 9.1976 - mae: 9.1976 - val_loss: 10.5742 - val_mae: 10.5742\n",
      "Epoch 385/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 9.1211 - mae: 9.1211 - val_loss: 10.0336 - val_mae: 10.0336\n",
      "Epoch 386/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.9384 - mae: 8.9384 - val_loss: 10.1945 - val_mae: 10.1945\n",
      "Epoch 387/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.0355 - mae: 9.0355 - val_loss: 10.3116 - val_mae: 10.3116\n",
      "Epoch 388/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.1495 - mae: 9.1495 - val_loss: 10.2661 - val_mae: 10.2661\n",
      "Epoch 389/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 9.0720 - mae: 9.0720 - val_loss: 10.2115 - val_mae: 10.2115\n",
      "Epoch 390/500\n",
      "510300/510300 [==============================] - 31s 61us/step - loss: 9.0255 - mae: 9.0255 - val_loss: 10.2704 - val_mae: 10.2704\n",
      "Epoch 391/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 9.0483 - mae: 9.0483 - val_loss: 10.0871 - val_mae: 10.0871\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9433 - mae: 8.9433 - val_loss: 10.3603 - val_mae: 10.3603\n",
      "Epoch 393/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 9.0341 - mae: 9.0341 - val_loss: 9.9456 - val_mae: 9.9456\n",
      "Epoch 394/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9883 - mae: 8.9883 - val_loss: 10.1815 - val_mae: 10.1815\n",
      "Epoch 395/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9416 - mae: 8.9416 - val_loss: 10.1640 - val_mae: 10.1640\n",
      "Epoch 396/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9794 - mae: 8.9794 - val_loss: 10.0281 - val_mae: 10.0281\n",
      "Epoch 397/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.8909 - mae: 8.8909 - val_loss: 10.1991 - val_mae: 10.1991\n",
      "Epoch 398/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9982 - mae: 8.9982 - val_loss: 10.1661 - val_mae: 10.1661\n",
      "Epoch 399/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9624 - mae: 8.9624 - val_loss: 10.1784 - val_mae: 10.1784\n",
      "Epoch 400/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9431 - mae: 8.9431 - val_loss: 9.9684 - val_mae: 9.9684\n",
      "Epoch 401/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.9842 - mae: 8.9842 - val_loss: 10.0868 - val_mae: 10.0868\n",
      "Epoch 402/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9627 - mae: 8.9627 - val_loss: 10.0835 - val_mae: 10.0835\n",
      "Epoch 403/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9063 - mae: 8.9063 - val_loss: 9.8713 - val_mae: 9.8713\n",
      "Epoch 404/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9310 - mae: 8.9310 - val_loss: 10.4913 - val_mae: 10.4913\n",
      "Epoch 405/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 9.1302 - mae: 9.1302 - val_loss: 10.0150 - val_mae: 10.0150\n",
      "Epoch 406/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.8896 - mae: 8.8896 - val_loss: 9.9736 - val_mae: 9.9736\n",
      "Epoch 407/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.9053 - mae: 8.9053 - val_loss: 10.0678 - val_mae: 10.0678\n",
      "Epoch 408/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9279 - mae: 8.9279 - val_loss: 10.3619 - val_mae: 10.3619\n",
      "Epoch 409/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9040 - mae: 8.9040 - val_loss: 9.8692 - val_mae: 9.8692\n",
      "Epoch 410/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.8958 - mae: 8.8958 - val_loss: 10.0705 - val_mae: 10.0705\n",
      "Epoch 411/500\n",
      "510300/510300 [==============================] - 41s 80us/step - loss: 8.8791 - mae: 8.8791 - val_loss: 9.9767 - val_mae: 9.9767\n",
      "Epoch 412/500\n",
      "510300/510300 [==============================] - 30s 59us/step - loss: 8.8736 - mae: 8.8736 - val_loss: 9.9508 - val_mae: 9.9508\n",
      "Epoch 413/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.8531 - mae: 8.8531 - val_loss: 10.2336 - val_mae: 10.2336\n",
      "Epoch 414/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.9187 - mae: 8.9187 - val_loss: 10.0343 - val_mae: 10.0343\n",
      "Epoch 415/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.8832 - mae: 8.8832 - val_loss: 9.9982 - val_mae: 9.9982\n",
      "Epoch 416/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.8153 - mae: 8.8153 - val_loss: 10.1731 - val_mae: 10.1731\n",
      "Epoch 417/500\n",
      "510300/510300 [==============================] - 25s 48us/step - loss: 8.8607 - mae: 8.8607 - val_loss: 10.0087 - val_mae: 10.0087\n",
      "Epoch 418/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.8265 - mae: 8.8265 - val_loss: 10.0120 - val_mae: 10.0120\n",
      "Epoch 419/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.8281 - mae: 8.8281 - val_loss: 10.2539 - val_mae: 10.2539\n",
      "Epoch 420/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 8.8797 - mae: 8.8797 - val_loss: 9.8296 - val_mae: 9.8296\n",
      "Epoch 421/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 8.7546 - mae: 8.7546 - val_loss: 10.3016 - val_mae: 10.3016\n",
      "Epoch 422/500\n",
      "510300/510300 [==============================] - 29s 56us/step - loss: 8.8874 - mae: 8.8874 - val_loss: 10.1047 - val_mae: 10.1047\n",
      "Epoch 423/500\n",
      "510300/510300 [==============================] - 27s 53us/step - loss: 8.8823 - mae: 8.8823 - val_loss: 9.8231 - val_mae: 9.8231\n",
      "Epoch 424/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 8.7842 - mae: 8.7842 - val_loss: 9.8055 - val_mae: 9.8055\n",
      "Epoch 425/500\n",
      "510300/510300 [==============================] - 31s 61us/step - loss: 8.8272 - mae: 8.8272 - val_loss: 9.6904 - val_mae: 9.6904\n",
      "Epoch 426/500\n",
      "510300/510300 [==============================] - 28s 55us/step - loss: 8.7843 - mae: 8.7843 - val_loss: 9.7896 - val_mae: 9.7896\n",
      "Epoch 427/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 8.6760 - mae: 8.6760 - val_loss: 10.0812 - val_mae: 10.0812\n",
      "Epoch 428/500\n",
      "510300/510300 [==============================] - 28s 54us/step - loss: 8.8649 - mae: 8.8649 - val_loss: 10.4723 - val_mae: 10.4723\n",
      "Epoch 429/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.8945 - mae: 8.8945 - val_loss: 9.9833 - val_mae: 9.9833\n",
      "Epoch 430/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.7872 - mae: 8.7872 - val_loss: 9.8426 - val_mae: 9.8426\n",
      "Epoch 431/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.7736 - mae: 8.7736 - val_loss: 9.8485 - val_mae: 9.8484\n",
      "Epoch 432/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 8.6663 - mae: 8.6663 - val_loss: 10.1681 - val_mae: 10.1681\n",
      "Epoch 433/500\n",
      "510300/510300 [==============================] - 32s 62us/step - loss: 8.7703 - mae: 8.7703 - val_loss: 9.7815 - val_mae: 9.7815\n",
      "Epoch 434/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.6708 - mae: 8.6708 - val_loss: 9.8306 - val_mae: 9.8306\n",
      "Epoch 435/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.7718 - mae: 8.7718 - val_loss: 10.1074 - val_mae: 10.1074\n",
      "Epoch 436/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.7175 - mae: 8.7175 - val_loss: 9.6772 - val_mae: 9.6772\n",
      "Epoch 437/500\n",
      "510300/510300 [==============================] - 26s 52us/step - loss: 8.6395 - mae: 8.6395 - val_loss: 9.9043 - val_mae: 9.9043\n",
      "Epoch 438/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.6845 - mae: 8.6845 - val_loss: 9.8669 - val_mae: 9.8669\n",
      "Epoch 439/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.7500 - mae: 8.7500 - val_loss: 9.8553 - val_mae: 9.8553\n",
      "Epoch 440/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.6917 - mae: 8.6917 - val_loss: 9.8660 - val_mae: 9.8660\n",
      "Epoch 441/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.7064 - mae: 8.7064 - val_loss: 9.5754 - val_mae: 9.5754\n",
      "Epoch 442/500\n",
      "510300/510300 [==============================] - 29s 57us/step - loss: 8.6291 - mae: 8.6291 - val_loss: 10.2080 - val_mae: 10.2080\n",
      "Epoch 443/500\n",
      "510300/510300 [==============================] - 42s 83us/step - loss: 8.7406 - mae: 8.7406 - val_loss: 9.8338 - val_mae: 9.8338\n",
      "Epoch 444/500\n",
      "510300/510300 [==============================] - 30s 59us/step - loss: 8.6407 - mae: 8.6407 - val_loss: 9.9733 - val_mae: 9.9733\n",
      "Epoch 445/500\n",
      "510300/510300 [==============================] - 27s 54us/step - loss: 8.7040 - mae: 8.7040 - val_loss: 9.8595 - val_mae: 9.8595\n",
      "Epoch 446/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.7419 - mae: 8.7419 - val_loss: 9.7820 - val_mae: 9.7820\n",
      "Epoch 447/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.6219 - mae: 8.6219 - val_loss: 9.7019 - val_mae: 9.7019\n",
      "Epoch 448/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.6516 - mae: 8.6516 - val_loss: 9.5943 - val_mae: 9.5943\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5796 - mae: 8.5796 - val_loss: 9.9413 - val_mae: 9.9413\n",
      "Epoch 450/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.6476 - mae: 8.6476 - val_loss: 9.7298 - val_mae: 9.7298\n",
      "Epoch 451/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.6592 - mae: 8.6592 - val_loss: 9.8212 - val_mae: 9.8212\n",
      "Epoch 452/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5753 - mae: 8.5753 - val_loss: 10.0647 - val_mae: 10.0647\n",
      "Epoch 453/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.6393 - mae: 8.6393 - val_loss: 9.6891 - val_mae: 9.6891\n",
      "Epoch 454/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.6199 - mae: 8.6199 - val_loss: 9.7162 - val_mae: 9.7162\n",
      "Epoch 455/500\n",
      "510300/510300 [==============================] - 27s 52us/step - loss: 8.5988 - mae: 8.5988 - val_loss: 9.8007 - val_mae: 9.8007\n",
      "Epoch 456/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.6788 - mae: 8.6788 - val_loss: 9.6957 - val_mae: 9.6957\n",
      "Epoch 457/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.6160 - mae: 8.6160 - val_loss: 9.6806 - val_mae: 9.6806\n",
      "Epoch 458/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5476 - mae: 8.5476 - val_loss: 9.6307 - val_mae: 9.6307\n",
      "Epoch 459/500\n",
      "510300/510300 [==============================] - 36s 70us/step - loss: 8.6373 - mae: 8.6373 - val_loss: 9.7313 - val_mae: 9.7313\n",
      "Epoch 460/500\n",
      "510300/510300 [==============================] - 35s 69us/step - loss: 8.5142 - mae: 8.5142 - val_loss: 9.5617 - val_mae: 9.5617\n",
      "Epoch 461/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.5920 - mae: 8.5920 - val_loss: 9.6866 - val_mae: 9.6866\n",
      "Epoch 462/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.5843 - mae: 8.5843 - val_loss: 9.6924 - val_mae: 9.6924\n",
      "Epoch 463/500\n",
      "510300/510300 [==============================] - 26s 52us/step - loss: 8.6883 - mae: 8.6883 - val_loss: 9.5747 - val_mae: 9.5747\n",
      "Epoch 464/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5314 - mae: 8.5314 - val_loss: 9.6890 - val_mae: 9.6890\n",
      "Epoch 465/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4942 - mae: 8.4942 - val_loss: 9.6722 - val_mae: 9.6722\n",
      "Epoch 466/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.5477 - mae: 8.5477 - val_loss: 9.8345 - val_mae: 9.8345\n",
      "Epoch 467/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.5670 - mae: 8.5670 - val_loss: 10.0655 - val_mae: 10.0655\n",
      "Epoch 468/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.6394 - mae: 8.6394 - val_loss: 9.5620 - val_mae: 9.5620\n",
      "Epoch 469/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4739 - mae: 8.4739 - val_loss: 9.8412 - val_mae: 9.8412\n",
      "Epoch 470/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5481 - mae: 8.5481 - val_loss: 9.5616 - val_mae: 9.5616\n",
      "Epoch 471/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5166 - mae: 8.5166 - val_loss: 9.5468 - val_mae: 9.5468\n",
      "Epoch 472/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4552 - mae: 8.4552 - val_loss: 9.8887 - val_mae: 9.8887\n",
      "Epoch 473/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.6220 - mae: 8.6220 - val_loss: 9.6300 - val_mae: 9.6300\n",
      "Epoch 474/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4925 - mae: 8.4925 - val_loss: 9.5493 - val_mae: 9.5493\n",
      "Epoch 475/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4369 - mae: 8.4369 - val_loss: 9.7301 - val_mae: 9.7301\n",
      "Epoch 476/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4675 - mae: 8.4675 - val_loss: 9.5451 - val_mae: 9.5451\n",
      "Epoch 477/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4424 - mae: 8.4424 - val_loss: 9.5522 - val_mae: 9.5522\n",
      "Epoch 478/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.4982 - mae: 8.4982 - val_loss: 9.7019 - val_mae: 9.7019\n",
      "Epoch 479/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4953 - mae: 8.4953 - val_loss: 9.7610 - val_mae: 9.7610\n",
      "Epoch 480/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.5094 - mae: 8.5094 - val_loss: 9.5011 - val_mae: 9.5011\n",
      "Epoch 481/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4655 - mae: 8.4655 - val_loss: 9.7945 - val_mae: 9.7945\n",
      "Epoch 482/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.5064 - mae: 8.5064 - val_loss: 9.4255 - val_mae: 9.4255\n",
      "Epoch 483/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.4139 - mae: 8.4139 - val_loss: 9.6515 - val_mae: 9.6515\n",
      "Epoch 484/500\n",
      "510300/510300 [==============================] - 26s 50us/step - loss: 8.5561 - mae: 8.5561 - val_loss: 9.7350 - val_mae: 9.7350\n",
      "Epoch 485/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.4524 - mae: 8.4524 - val_loss: 9.3808 - val_mae: 9.3808\n",
      "Epoch 486/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3875 - mae: 8.3875 - val_loss: 9.6305 - val_mae: 9.6305\n",
      "Epoch 487/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.4326 - mae: 8.4326 - val_loss: 9.4122 - val_mae: 9.4122\n",
      "Epoch 488/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3165 - mae: 8.3165 - val_loss: 9.2528 - val_mae: 9.2528\n",
      "Epoch 489/500\n",
      "510300/510300 [==============================] - 26s 51us/step - loss: 8.3860 - mae: 8.3860 - val_loss: 9.4849 - val_mae: 9.4849\n",
      "Epoch 490/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3808 - mae: 8.3808 - val_loss: 9.2322 - val_mae: 9.2322\n",
      "Epoch 491/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.3652 - mae: 8.3652 - val_loss: 9.5735 - val_mae: 9.5735\n",
      "Epoch 492/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3979 - mae: 8.3979 - val_loss: 9.5054 - val_mae: 9.5054\n",
      "Epoch 493/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3821 - mae: 8.3821 - val_loss: 9.6982 - val_mae: 9.6982\n",
      "Epoch 494/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3818 - mae: 8.3818 - val_loss: 9.4619 - val_mae: 9.4619\n",
      "Epoch 495/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.3846 - mae: 8.3846 - val_loss: 9.8694 - val_mae: 9.8694\n",
      "Epoch 496/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.3879 - mae: 8.3879 - val_loss: 9.4628 - val_mae: 9.4628\n",
      "Epoch 497/500\n",
      "510300/510300 [==============================] - 25s 50us/step - loss: 8.3851 - mae: 8.3851 - val_loss: 9.3471 - val_mae: 9.3471\n",
      "Epoch 498/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.3977 - mae: 8.3977 - val_loss: 9.5840 - val_mae: 9.5840\n",
      "Epoch 499/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.3950 - mae: 8.3950 - val_loss: 9.4200 - val_mae: 9.4200\n",
      "Epoch 500/500\n",
      "510300/510300 [==============================] - 25s 49us/step - loss: 8.3228 - mae: 8.3228 - val_loss: 9.3743 - val_mae: 9.3743\n"
     ]
    }
   ],
   "source": [
    "mod=model.fit(train_X, train_Y, epochs=500, batch_size=10000, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(mod.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(mod.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243000/243000 [==============================] - 9s 36us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.378753410935893, 9.378752708435059]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_X, valid_Y, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243000/243000 [==============================] - 16s 67us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.378753410935893, 9.378752708435059]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_X, valid_Y, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission 파일을 생성합니다.\n",
    "sample_sub = pd.read_csv('sample_submission.csv', index_col=0)\n",
    "submission = sample_sub+pred_test\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(num_layers,epoch,batch,learnrate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=160, input_dim=226, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    for i in range(int(num_layers)-1):\n",
    "        model.add(Dense(units=160, kernel_initializer='he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(units=4, kernel_initializer='he_normal', activation='linear'))\n",
    "    \n",
    "    adam=optimizers.Adam(lr=learnrate)\n",
    "    model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    mod=model.fit(train_X, train_Y, epochs=int(epoch), batch_size=int(batch), validation_split=0.1, verbose=2)\n",
    "    \n",
    "    score=model.evaluate(valid_X, valid_Y, batch_size=int(batch))\n",
    "    print('Valid loss:',score[0])\n",
    "    print('Valid accuracy:',score[1])\n",
    "    \n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   batch   |   epoch   | learnrate | num_la... |\n",
      "-------------------------------------------------------------------------\n",
      "Train on 510300 samples, validate on 56700 samples\n",
      "Epoch 1/1572\n",
      " - 28s - loss: 120.3618 - accuracy: 0.2690 - val_loss: 137.8309 - val_accuracy: 0.2547\n",
      "Epoch 2/1572\n",
      " - 26s - loss: 60.6834 - accuracy: 0.3211 - val_loss: 83.0253 - val_accuracy: 0.2526\n",
      "Epoch 3/1572\n",
      " - 26s - loss: 46.2783 - accuracy: 0.4833 - val_loss: 82.2381 - val_accuracy: 0.3271\n",
      "Epoch 4/1572\n",
      " - 27s - loss: 30.1542 - accuracy: 0.6967 - val_loss: 63.4381 - val_accuracy: 0.4071\n",
      "Epoch 5/1572\n",
      " - 27s - loss: 21.5195 - accuracy: 0.7775 - val_loss: 49.0281 - val_accuracy: 0.4992\n",
      "Epoch 6/1572\n",
      " - 27s - loss: 17.9361 - accuracy: 0.8104 - val_loss: 36.5983 - val_accuracy: 0.6487\n",
      "Epoch 7/1572\n",
      " - 27s - loss: 15.7433 - accuracy: 0.8315 - val_loss: 28.2864 - val_accuracy: 0.7160\n",
      "Epoch 8/1572\n",
      " - 26s - loss: 14.3287 - accuracy: 0.8464 - val_loss: 23.2371 - val_accuracy: 0.7635\n",
      "Epoch 9/1572\n",
      " - 25s - loss: 13.2741 - accuracy: 0.8561 - val_loss: 19.9043 - val_accuracy: 0.7893\n",
      "Epoch 10/1572\n",
      " - 26s - loss: 12.5065 - accuracy: 0.8640 - val_loss: 17.6661 - val_accuracy: 0.8171\n",
      "Epoch 11/1572\n",
      " - 25s - loss: 11.8775 - accuracy: 0.8708 - val_loss: 16.4161 - val_accuracy: 0.8212\n",
      "Epoch 12/1572\n",
      " - 27s - loss: 11.3154 - accuracy: 0.8767 - val_loss: 14.6302 - val_accuracy: 0.8449\n",
      "Epoch 13/1572\n",
      " - 25s - loss: 10.8508 - accuracy: 0.8816 - val_loss: 13.3088 - val_accuracy: 0.8599\n",
      "Epoch 14/1572\n",
      " - 25s - loss: 10.4714 - accuracy: 0.8856 - val_loss: 12.7110 - val_accuracy: 0.8625\n",
      "Epoch 15/1572\n",
      " - 27s - loss: 10.1265 - accuracy: 0.8889 - val_loss: 12.2058 - val_accuracy: 0.8648\n",
      "Epoch 16/1572\n",
      " - 26s - loss: 9.8255 - accuracy: 0.8913 - val_loss: 11.4569 - val_accuracy: 0.8779\n",
      "Epoch 17/1572\n",
      " - 30s - loss: 9.5491 - accuracy: 0.8949 - val_loss: 11.8265 - val_accuracy: 0.8789\n",
      "Epoch 18/1572\n",
      " - 29s - loss: 9.2726 - accuracy: 0.8980 - val_loss: 12.0469 - val_accuracy: 0.8665\n",
      "Epoch 19/1572\n",
      " - 26s - loss: 9.0884 - accuracy: 0.8995 - val_loss: 11.4410 - val_accuracy: 0.8721\n",
      "Epoch 20/1572\n",
      " - 31s - loss: 8.8459 - accuracy: 0.9022 - val_loss: 10.9485 - val_accuracy: 0.8773\n",
      "Epoch 21/1572\n",
      " - 26s - loss: 8.7334 - accuracy: 0.9039 - val_loss: 10.5468 - val_accuracy: 0.8881\n",
      "Epoch 22/1572\n",
      " - 25s - loss: 8.6220 - accuracy: 0.9042 - val_loss: 11.4758 - val_accuracy: 0.8813\n",
      "Epoch 23/1572\n",
      " - 27s - loss: 8.4500 - accuracy: 0.9059 - val_loss: 10.2512 - val_accuracy: 0.8831\n",
      "Epoch 24/1572\n",
      " - 36s - loss: 8.2407 - accuracy: 0.9085 - val_loss: 9.4751 - val_accuracy: 0.8937\n",
      "Epoch 25/1572\n",
      " - 47s - loss: 8.1180 - accuracy: 0.9099 - val_loss: 9.7479 - val_accuracy: 0.8927\n",
      "Epoch 26/1572\n",
      " - 37s - loss: 8.0088 - accuracy: 0.9108 - val_loss: 9.8311 - val_accuracy: 0.8951\n",
      "Epoch 27/1572\n",
      " - 37s - loss: 7.8134 - accuracy: 0.9125 - val_loss: 9.6731 - val_accuracy: 0.8839\n",
      "Epoch 28/1572\n",
      " - 27s - loss: 7.7854 - accuracy: 0.9130 - val_loss: 9.3265 - val_accuracy: 0.8895\n",
      "Epoch 29/1572\n",
      " - 26s - loss: 7.6001 - accuracy: 0.9145 - val_loss: 9.1725 - val_accuracy: 0.8940\n",
      "Epoch 30/1572\n",
      " - 26s - loss: 7.6009 - accuracy: 0.9151 - val_loss: 9.0215 - val_accuracy: 0.8977\n",
      "Epoch 31/1572\n",
      " - 26s - loss: 7.4723 - accuracy: 0.9163 - val_loss: 9.0570 - val_accuracy: 0.8911\n",
      "Epoch 32/1572\n",
      " - 26s - loss: 7.3872 - accuracy: 0.9171 - val_loss: 9.2062 - val_accuracy: 0.9013\n",
      "Epoch 33/1572\n",
      " - 26s - loss: 7.4079 - accuracy: 0.9161 - val_loss: 8.3783 - val_accuracy: 0.9028\n",
      "Epoch 34/1572\n",
      " - 26s - loss: 7.2049 - accuracy: 0.9191 - val_loss: 8.8329 - val_accuracy: 0.8938\n",
      "Epoch 35/1572\n",
      " - 26s - loss: 7.1575 - accuracy: 0.9196 - val_loss: 8.6199 - val_accuracy: 0.9067\n",
      "Epoch 36/1572\n",
      " - 32s - loss: 7.1516 - accuracy: 0.9192 - val_loss: 8.3394 - val_accuracy: 0.9019\n",
      "Epoch 37/1572\n",
      " - 32s - loss: 6.9774 - accuracy: 0.9209 - val_loss: 8.4848 - val_accuracy: 0.8994\n",
      "Epoch 38/1572\n",
      " - 26s - loss: 6.9705 - accuracy: 0.9209 - val_loss: 8.1141 - val_accuracy: 0.9054\n",
      "Epoch 39/1572\n",
      " - 26s - loss: 6.8941 - accuracy: 0.9218 - val_loss: 8.5877 - val_accuracy: 0.9065\n",
      "Epoch 40/1572\n",
      " - 26s - loss: 6.8564 - accuracy: 0.9221 - val_loss: 8.0425 - val_accuracy: 0.9133\n",
      "Epoch 41/1572\n",
      " - 26s - loss: 6.7842 - accuracy: 0.9232 - val_loss: 7.7235 - val_accuracy: 0.9140\n",
      "Epoch 42/1572\n",
      " - 26s - loss: 6.7196 - accuracy: 0.9234 - val_loss: 7.7994 - val_accuracy: 0.9113\n",
      "Epoch 43/1572\n",
      " - 26s - loss: 6.6744 - accuracy: 0.9238 - val_loss: 8.2958 - val_accuracy: 0.8960\n",
      "Epoch 44/1572\n",
      " - 26s - loss: 6.6195 - accuracy: 0.9246 - val_loss: 8.0479 - val_accuracy: 0.9019\n",
      "Epoch 45/1572\n",
      " - 25s - loss: 6.5713 - accuracy: 0.9255 - val_loss: 7.5064 - val_accuracy: 0.9162\n",
      "Epoch 46/1572\n",
      " - 26s - loss: 6.5273 - accuracy: 0.9254 - val_loss: 7.7179 - val_accuracy: 0.9140\n",
      "Epoch 47/1572\n",
      " - 25s - loss: 6.4608 - accuracy: 0.9260 - val_loss: 8.0031 - val_accuracy: 0.9063\n",
      "Epoch 48/1572\n",
      " - 26s - loss: 6.4354 - accuracy: 0.9263 - val_loss: 7.4843 - val_accuracy: 0.9157\n",
      "Epoch 49/1572\n",
      " - 26s - loss: 6.3596 - accuracy: 0.9271 - val_loss: 7.4716 - val_accuracy: 0.9103\n",
      "Epoch 50/1572\n",
      " - 26s - loss: 6.3468 - accuracy: 0.9271 - val_loss: 7.6572 - val_accuracy: 0.9053\n",
      "Epoch 51/1572\n",
      " - 26s - loss: 6.2641 - accuracy: 0.9273 - val_loss: 7.2579 - val_accuracy: 0.9186\n",
      "Epoch 52/1572\n",
      " - 26s - loss: 6.2412 - accuracy: 0.9277 - val_loss: 7.4715 - val_accuracy: 0.9183\n",
      "Epoch 53/1572\n",
      " - 26s - loss: 6.2423 - accuracy: 0.9279 - val_loss: 7.4546 - val_accuracy: 0.9060\n",
      "Epoch 54/1572\n",
      " - 25s - loss: 6.1890 - accuracy: 0.9285 - val_loss: 7.5682 - val_accuracy: 0.9123\n",
      "Epoch 55/1572\n",
      " - 26s - loss: 6.1204 - accuracy: 0.9293 - val_loss: 7.1824 - val_accuracy: 0.9157\n",
      "Epoch 56/1572\n",
      " - 26s - loss: 6.1156 - accuracy: 0.9293 - val_loss: 7.2469 - val_accuracy: 0.9141\n",
      "Epoch 57/1572\n",
      " - 25s - loss: 6.1530 - accuracy: 0.9291 - val_loss: 7.4254 - val_accuracy: 0.9116\n",
      "Epoch 58/1572\n",
      " - 26s - loss: 6.0628 - accuracy: 0.9303 - val_loss: 7.3505 - val_accuracy: 0.9098\n",
      "Epoch 59/1572\n",
      " - 26s - loss: 6.0215 - accuracy: 0.9306 - val_loss: 6.9158 - val_accuracy: 0.9276\n",
      "Epoch 60/1572\n",
      " - 26s - loss: 5.9976 - accuracy: 0.9304 - val_loss: 7.1364 - val_accuracy: 0.9195\n",
      "Epoch 61/1572\n",
      " - 27s - loss: 5.9571 - accuracy: 0.9307 - val_loss: 7.4278 - val_accuracy: 0.9209\n",
      "Epoch 62/1572\n",
      " - 33s - loss: 5.9047 - accuracy: 0.9315 - val_loss: 6.6685 - val_accuracy: 0.9267\n",
      "Epoch 63/1572\n",
      " - 44s - loss: 5.8546 - accuracy: 0.9323 - val_loss: 6.7080 - val_accuracy: 0.9164\n",
      "Epoch 64/1572\n",
      " - 65s - loss: 5.8666 - accuracy: 0.9323 - val_loss: 6.6965 - val_accuracy: 0.9249\n",
      "Epoch 65/1572\n",
      " - 58s - loss: 5.7927 - accuracy: 0.9332 - val_loss: 6.8997 - val_accuracy: 0.9141\n",
      "Epoch 66/1572\n",
      " - 58s - loss: 5.8430 - accuracy: 0.9318 - val_loss: 6.7367 - val_accuracy: 0.9194\n",
      "Epoch 67/1572\n",
      " - 70s - loss: 5.7845 - accuracy: 0.9327 - val_loss: 6.4872 - val_accuracy: 0.9211\n",
      "Epoch 68/1572\n",
      " - 32s - loss: 5.7759 - accuracy: 0.9332 - val_loss: 6.8828 - val_accuracy: 0.9238\n",
      "Epoch 69/1572\n",
      " - 32s - loss: 5.7080 - accuracy: 0.9338 - val_loss: 6.9514 - val_accuracy: 0.9195\n",
      "Epoch 70/1572\n",
      " - 42s - loss: 5.6962 - accuracy: 0.9335 - val_loss: 6.8322 - val_accuracy: 0.9224\n",
      "Epoch 71/1572\n",
      " - 48s - loss: 5.6556 - accuracy: 0.9339 - val_loss: 6.5108 - val_accuracy: 0.9287\n",
      "Epoch 72/1572\n",
      " - 51s - loss: 5.6143 - accuracy: 0.9346 - val_loss: 6.9688 - val_accuracy: 0.9160\n",
      "Epoch 73/1572\n",
      " - 37s - loss: 5.6297 - accuracy: 0.9341 - val_loss: 6.5795 - val_accuracy: 0.9216\n",
      "Epoch 74/1572\n",
      " - 27s - loss: 5.6527 - accuracy: 0.9343 - val_loss: 6.3999 - val_accuracy: 0.9255\n",
      "Epoch 75/1572\n",
      " - 27s - loss: 5.5552 - accuracy: 0.9350 - val_loss: 6.7063 - val_accuracy: 0.9223\n",
      "Epoch 76/1572\n",
      " - 26s - loss: 5.5786 - accuracy: 0.9346 - val_loss: 6.8278 - val_accuracy: 0.9213\n",
      "Epoch 77/1572\n",
      " - 26s - loss: 5.5249 - accuracy: 0.9357 - val_loss: 6.5357 - val_accuracy: 0.9213\n",
      "Epoch 78/1572\n",
      " - 26s - loss: 5.4891 - accuracy: 0.9355 - val_loss: 6.6309 - val_accuracy: 0.9274\n",
      "Epoch 79/1572\n",
      " - 26s - loss: 5.4783 - accuracy: 0.9357 - val_loss: 6.2500 - val_accuracy: 0.9235\n",
      "Epoch 80/1572\n",
      " - 26s - loss: 5.5270 - accuracy: 0.9353 - val_loss: 6.2253 - val_accuracy: 0.9291\n",
      "Epoch 81/1572\n",
      " - 26s - loss: 5.4924 - accuracy: 0.9357 - val_loss: 6.1675 - val_accuracy: 0.9285\n",
      "Epoch 82/1572\n",
      " - 32s - loss: 5.4040 - accuracy: 0.9370 - val_loss: 6.3179 - val_accuracy: 0.9327\n",
      "Epoch 83/1572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 34s - loss: 5.4239 - accuracy: 0.9366 - val_loss: 6.2811 - val_accuracy: 0.9241\n",
      "Epoch 84/1572\n",
      " - 26s - loss: 5.3760 - accuracy: 0.9367 - val_loss: 6.1211 - val_accuracy: 0.9277\n",
      "Epoch 85/1572\n",
      " - 26s - loss: 5.3185 - accuracy: 0.9368 - val_loss: 6.3837 - val_accuracy: 0.9325\n",
      "Epoch 86/1572\n",
      " - 27s - loss: 5.3360 - accuracy: 0.9372 - val_loss: 6.5472 - val_accuracy: 0.9232\n",
      "Epoch 87/1572\n",
      " - 26s - loss: 5.3582 - accuracy: 0.9365 - val_loss: 6.2954 - val_accuracy: 0.9357\n",
      "Epoch 88/1572\n",
      " - 26s - loss: 5.3232 - accuracy: 0.9374 - val_loss: 6.3945 - val_accuracy: 0.9286\n",
      "Epoch 89/1572\n",
      " - 27s - loss: 5.3368 - accuracy: 0.9370 - val_loss: 6.4011 - val_accuracy: 0.9159\n",
      "Epoch 90/1572\n",
      " - 43s - loss: 5.2774 - accuracy: 0.9378 - val_loss: 6.4084 - val_accuracy: 0.9227\n",
      "Epoch 91/1572\n",
      " - 42s - loss: 5.2790 - accuracy: 0.9381 - val_loss: 6.3127 - val_accuracy: 0.9312\n",
      "Epoch 92/1572\n",
      " - 38s - loss: 5.2620 - accuracy: 0.9377 - val_loss: 6.1718 - val_accuracy: 0.9235\n",
      "Epoch 93/1572\n",
      " - 27s - loss: 5.1816 - accuracy: 0.9382 - val_loss: 6.3305 - val_accuracy: 0.9314\n",
      "Epoch 94/1572\n",
      " - 26s - loss: 5.2043 - accuracy: 0.9383 - val_loss: 6.6624 - val_accuracy: 0.9254\n",
      "Epoch 95/1572\n",
      " - 26s - loss: 5.2658 - accuracy: 0.9380 - val_loss: 6.4954 - val_accuracy: 0.9237\n",
      "Epoch 96/1572\n",
      " - 26s - loss: 5.2112 - accuracy: 0.9385 - val_loss: 6.0319 - val_accuracy: 0.9338\n",
      "Epoch 97/1572\n",
      " - 30s - loss: 5.1537 - accuracy: 0.9390 - val_loss: 5.9120 - val_accuracy: 0.9249\n",
      "Epoch 98/1572\n",
      " - 34s - loss: 5.1847 - accuracy: 0.9383 - val_loss: 6.0875 - val_accuracy: 0.9341\n",
      "Epoch 99/1572\n",
      " - 27s - loss: 5.1378 - accuracy: 0.9394 - val_loss: 5.9440 - val_accuracy: 0.9319\n",
      "Epoch 100/1572\n",
      " - 29s - loss: 5.1327 - accuracy: 0.9392 - val_loss: 6.1577 - val_accuracy: 0.9354\n",
      "Epoch 101/1572\n",
      " - 44s - loss: 5.1551 - accuracy: 0.9392 - val_loss: 5.9436 - val_accuracy: 0.9310\n",
      "Epoch 102/1572\n",
      " - 36s - loss: 5.0632 - accuracy: 0.9396 - val_loss: 5.7103 - val_accuracy: 0.9314\n",
      "Epoch 103/1572\n",
      " - 29s - loss: 5.1067 - accuracy: 0.9395 - val_loss: 6.6790 - val_accuracy: 0.9209\n",
      "Epoch 104/1572\n",
      " - 29s - loss: 5.1398 - accuracy: 0.9390 - val_loss: 5.7538 - val_accuracy: 0.9266\n",
      "Epoch 105/1572\n",
      " - 26s - loss: 5.0674 - accuracy: 0.9398 - val_loss: 5.9763 - val_accuracy: 0.9295\n",
      "Epoch 106/1572\n",
      " - 26s - loss: 5.0999 - accuracy: 0.9394 - val_loss: 5.9493 - val_accuracy: 0.9339\n",
      "Epoch 107/1572\n",
      " - 26s - loss: 5.0463 - accuracy: 0.9397 - val_loss: 5.8810 - val_accuracy: 0.9345\n",
      "Epoch 108/1572\n",
      " - 27s - loss: 5.0073 - accuracy: 0.9406 - val_loss: 6.0145 - val_accuracy: 0.9286\n",
      "Epoch 109/1572\n",
      " - 26s - loss: 5.0020 - accuracy: 0.9403 - val_loss: 5.8763 - val_accuracy: 0.9297\n",
      "Epoch 110/1572\n",
      " - 26s - loss: 5.0312 - accuracy: 0.9403 - val_loss: 5.8614 - val_accuracy: 0.9291\n",
      "Epoch 111/1572\n",
      " - 26s - loss: 4.9891 - accuracy: 0.9403 - val_loss: 5.6124 - val_accuracy: 0.9389\n",
      "Epoch 112/1572\n",
      " - 31s - loss: 4.9901 - accuracy: 0.9407 - val_loss: 5.8851 - val_accuracy: 0.9328\n",
      "Epoch 113/1572\n",
      " - 26s - loss: 4.9728 - accuracy: 0.9406 - val_loss: 5.8801 - val_accuracy: 0.9329\n",
      "Epoch 114/1572\n",
      " - 26s - loss: 4.9956 - accuracy: 0.9408 - val_loss: 5.6893 - val_accuracy: 0.9343\n",
      "Epoch 115/1572\n",
      " - 26s - loss: 4.9255 - accuracy: 0.9413 - val_loss: 5.6155 - val_accuracy: 0.9319\n",
      "Epoch 116/1572\n",
      " - 26s - loss: 4.8779 - accuracy: 0.9414 - val_loss: 5.9953 - val_accuracy: 0.9330\n",
      "Epoch 117/1572\n",
      " - 26s - loss: 4.9273 - accuracy: 0.9409 - val_loss: 5.5461 - val_accuracy: 0.9348\n",
      "Epoch 118/1572\n",
      " - 27s - loss: 4.8699 - accuracy: 0.9415 - val_loss: 5.9174 - val_accuracy: 0.9332\n",
      "Epoch 119/1572\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (5939.321535345923, 1572.7840495586292, 0.006067357423109275, 5.448831829968968)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-e7b240728dbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-d64846267643>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[1;34m(num_layers, epoch, batch, learnrate)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    182\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 184\u001b[1;33m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0m\u001b[0;32m    185\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "pb={'num_layers':(0,10),'epoch':(500,2000),'batch':(1000,10000),'learnrate':(1e-4, 1e-2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_and_validate,\n",
    "    pbounds=pb,\n",
    "    random_state=0,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10,n_iter=30)\n",
    "\n",
    "for i,res in enumerate(optimizer.res):\n",
    "    print('Iteration {}: \\n\\t{}'.format(i,res))\n",
    "print()\n",
    "print()\n",
    "print('Final result: ',optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
